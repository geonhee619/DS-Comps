{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from logging import DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "from logging import getLogger, StreamHandler, FileHandler, Formatter\n",
    "from contextlib import contextmanager\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler, MinMaxScaler\n",
    "from category_encoders import OrdinalEncoder, OneHotEncoder, TargetEncoder\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from gensim.models import Word2Vec\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, average_precision_score, log_loss, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from scipy.optimize import minimize\n",
    " \n",
    "import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import torch\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('max_rows', 1000)\n",
    "pd.set_option('max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \n",
    "def get_logger(\n",
    "    filename='log',\n",
    "    disable_stream_handler=False,\n",
    "    disable_file_handler=False,\n",
    "    level=20, # INFO\n",
    "    log_format=\"%(message)s\",\n",
    "):\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    if not disable_stream_handler:\n",
    "        handler1 = StreamHandler()\n",
    "        handler1.setFormatter(Formatter(log_format))\n",
    "        logger.addHandler(handler1)\n",
    "    \n",
    "    if not disable_file_handler:\n",
    "        handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "        handler2.setFormatter(Formatter(log_format))\n",
    "        logger.addHandler(handler2)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "\n",
    "def printl(msg, level=20):\n",
    "    # DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "    # (10, 20, 30, 40, 50)\n",
    "    try:\n",
    "        if level == 10:\n",
    "            logger.debug(msg)\n",
    "        elif level == 20:\n",
    "            logger.info(msg)\n",
    "        elif level == 30:\n",
    "            logger.warning(msg)\n",
    "        elif level == 40:\n",
    "            logger.error(msg)\n",
    "        elif level == 50:\n",
    "            logger.critical(msg)\n",
    "        else:\n",
    "            raise ValueError\n",
    "    except NameError:\n",
    "        print(msg)\n",
    "        \n",
    "        \n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    printl(f\"[{name}] done in {time.time() - t0:.0f} s\")\n",
    "    \n",
    "    \n",
    "def load_df(path):\n",
    "    basename = os.path.basename(path)\n",
    "    ext = path.split('.')[-1]\n",
    "    \n",
    "    if ext == 'csv':\n",
    "        df = pd.read_csv(path)\n",
    "    elif ext == 'pkl':\n",
    "        df = pd.read_pickle(path)\n",
    "    else:\n",
    "        raise IOError(f'Not Accessable Format .{ext}')\n",
    "\n",
    "    printl(f\"{basename} shape / {df.shape}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_target_encoding(\n",
    "    train,\n",
    "    target,\n",
    "    test,\n",
    "    cv,\n",
    "    cols,\n",
    "    handle_missing='value',\n",
    "    handle_unknown='value',\n",
    "    min_samples_leaf=1,\n",
    "    smoothing=1.0,\n",
    "    agg='mean'\n",
    "):\n",
    "    encoded_cols = []\n",
    "    for c in cols:\n",
    "        ecol = f'TE_{agg}_{c}'\n",
    "        train[ecol] = np.nan\n",
    "        encoded_cols.append(ecol)\n",
    "    \n",
    "    # TE for oof\n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        te = TargetEncoder(cols=cols,\n",
    "                           handle_missing=handle_missing,\n",
    "                           handle_unknown=handle_unknown,\n",
    "                           min_samples_leaf=min_samples_leaf,\n",
    "                           smoothing=smoothing)\n",
    "        te.fit(train.loc[dev_idx, cols], target[dev_idx], agg=agg)\n",
    "        train.loc[val_idx, encoded_cols] = te.transform(train.loc[val_idx, cols]).values\n",
    "    \n",
    "    # TE for test\n",
    "    te = TargetEncoder(cols=cols,\n",
    "                       handle_missing=handle_missing,\n",
    "                       handle_unknown=handle_unknown,\n",
    "                       min_samples_leaf=min_samples_leaf,\n",
    "                       smoothing=smoothing)\n",
    "    te.fit(train[cols], target)\n",
    "    test[encoded_cols] = te.transform(test[cols])\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "class NNPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_columns='auto',\n",
    "        minmax_columns='auto',\n",
    "        gaussian_columns='auto',\n",
    "        ignore_columns=[],\n",
    "        n_quantiles=1000,\n",
    "        max_iter=10,\n",
    "        random_state=42,\n",
    "    ):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.minmax_columns = minmax_columns\n",
    "        self.gaussian_columns = gaussian_columns\n",
    "        self.ignore_columns = ignore_columns\n",
    "\n",
    "        self.dummied_columns = []\n",
    "        self.has_nan_columns = []\n",
    "        self.nan_flag_columns = []\n",
    "        \n",
    "        self.oe = OneHotEncoder(cols=categorical_columns,\n",
    "                                handle_missing='indicator',\n",
    "                                handle_unknown='error',\n",
    "                                use_cat_names=True)\n",
    "        self.mme = MinMaxScaler()\n",
    "        self.qt = QuantileTransformer(n_quantiles=n_quantiles,\n",
    "                                      output_distribution='normal',\n",
    "                                      random_state=random_state)\n",
    "        self.imp = IterativeImputer(sample_posterior=True,\n",
    "                                    max_iter=max_iter,\n",
    "                                    random_state=random_state)\n",
    "    \n",
    "    def _set_columns(self, X):\n",
    "        if self.categorical_columns == 'auto':\n",
    "            columns = [c for c in X.columns if X.dtypes[c] == 'object']\n",
    "            columns = [c for c in columns if c not in self.ignore_columns]\n",
    "            self.categorical_columns = columns\n",
    "        \n",
    "        numerical_columns = [c for c in X.columns if c not in self.categorical_columns + self.ignore_columns]\n",
    "        \n",
    "        if self.minmax_columns == 'auto':\n",
    "            nunique = X[numerical_columns].nunique()\n",
    "            columns = nunique[nunique == 2].index.to_list()\n",
    "            self.minmax_columns = columns\n",
    "        \n",
    "        if self.gaussian_columns == 'auto':\n",
    "            columns = [c for c in numerical_columns if c not in self.minmax_columns]\n",
    "            self.gaussian_columns = columns\n",
    "        \n",
    "        nan_count = X[numerical_columns].isnull().sum()\n",
    "        columns = nan_count[nan_count > 0].index.to_list()\n",
    "        self.has_nan_columns = columns\n",
    "        self.nan_flag_columns = [f'{c}_nanflag' for c in self.has_nan_columns]\n",
    "        \n",
    "    def get_feature_columns(self):\n",
    "        columns = []\n",
    "        columns += self.dummied_columns\n",
    "        columns += self.minmax_columns\n",
    "        columns += self.gaussian_columns\n",
    "        columns += self.nan_flag_columns\n",
    "        return columns\n",
    "\n",
    "    def fit(self, X):\n",
    "        self._set_columns(X)\n",
    "        \n",
    "        # categorical encoding\n",
    "        if len(self.categorical_columns) > 0:\n",
    "            self.oe.fit(X[self.categorical_columns])\n",
    "            self.dummied_columns += self.oe.get_feature_names()\n",
    "        \n",
    "        # minmax encoding\n",
    "        if len(self.minmax_columns) > 0:\n",
    "            self.mme.fit(X[self.minmax_columns])\n",
    "        \n",
    "        # (rank-)gaussian encoding\n",
    "        if len(self.gaussian_columns) > 0:\n",
    "            self.qt.fit(X[self.gaussian_columns])\n",
    "            \n",
    "        # multiple imputation\n",
    "        if len(self.has_nan_columns) > 0:\n",
    "            columns = self.minmax_columns + self.gaussian_columns\n",
    "            self.imp.fit(X[columns])\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # categorical encoding\n",
    "        if len(self.categorical_columns) > 0:\n",
    "            X = pd.concat([X, self.oe.transform(X[self.categorical_columns])], axis=1)\n",
    "            X = X.drop(self.categorical_columns, axis=1)\n",
    "        \n",
    "        # minmax encoding\n",
    "        if len(self.minmax_columns) > 0:\n",
    "            X[self.minmax_columns] = self.mme.transform(X[self.minmax_columns])\n",
    "        \n",
    "        # (rank-)gaussian encoding\n",
    "        if len(self.gaussian_columns) > 0:\n",
    "            X[self.gaussian_columns] = self.qt.transform(X[self.gaussian_columns])\n",
    "        \n",
    "        # make nan flag dataframe\n",
    "        if len(self.has_nan_columns) > 0:\n",
    "            X_nan = X[self.has_nan_columns].copy()\n",
    "            X_nan = X_nan.isnull().astype(np.float32)\n",
    "            X_nan.columns = self.nan_flag_columns\n",
    "        \n",
    "            # multiple imputation\n",
    "            columns = self.minmax_columns + self.gaussian_columns\n",
    "            X[columns] = self.imp.transform(X[columns])\n",
    "            \n",
    "            # add nan dataframe\n",
    "            X = pd.concat([X, X_nan], axis=1)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGroupKFold:\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        groups = pd.Series(groups)\n",
    "        unique_groups = np.unique(groups)\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n",
    "        for tr_group_idx, va_group_idx in kf.split(unique_groups):\n",
    "            tr_groups, va_groups = unique_groups[tr_group_idx], unique_groups[va_group_idx]\n",
    "            tr_indices = groups[groups.isin(tr_groups)].index.to_list()\n",
    "            va_indices = groups[groups.isin(va_groups)].index.to_list()\n",
    "            yield tr_indices, va_indices\n",
    "            \n",
    "            \n",
    "class StratifiedGroupKFold:\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    # Implementation based on this kaggle kernel:\n",
    "    #    https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        k = self.n_splits\n",
    "        rnd = check_random_state(self.random_state)\n",
    "            \n",
    "        # labels_num: zero-origin number of label\n",
    "        # ex) unique = [0,1,2,3] -> labels_num = 4\n",
    "        labels_num = np.max(y) + 1\n",
    "        \n",
    "        # y_counts_per_group: in-group label distribution\n",
    "        # y_distr: whole label distribution\n",
    "        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "        y_distr = Counter()\n",
    "        for label, g in zip(y, groups):\n",
    "            y_counts_per_group[g][label] += 1\n",
    "            y_distr[label] += 1\n",
    "\n",
    "        # y_counts_per_fold: in-fold label distribution\n",
    "        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "        groups_per_fold = defaultdict(set)\n",
    "        \n",
    "        # return mean std of per label counts when y_counts is in fold\n",
    "        def eval_y_counts_per_fold(y_counts, fold):\n",
    "            y_counts_per_fold[fold] += y_counts\n",
    "            std_per_label = []\n",
    "            for label in range(labels_num):\n",
    "                label_std = np.std(\n",
    "                    [y_counts_per_fold[i][label] / y_distr[label] for i in range(k)]\n",
    "                )\n",
    "                std_per_label.append(label_std)\n",
    "            y_counts_per_fold[fold] -= y_counts\n",
    "            return np.mean(std_per_label)\n",
    "        \n",
    "        # list of [group, y_counts]\n",
    "        # if shuffle: fold changes in same np.std(y_counts)\n",
    "        # ascending groups by degree of label variance\n",
    "        groups_and_y_counts = list(y_counts_per_group.items())\n",
    "        if self.shuffle:\n",
    "            rnd.shuffle(groups_and_y_counts)\n",
    "        groups_and_y_counts = sorted(groups_and_y_counts, key=lambda x: -np.std(x[1]))\n",
    "\n",
    "        # set fold for each group such that label distirbution will be uniform\n",
    "        for g, y_counts in groups_and_y_counts:\n",
    "            best_fold = None\n",
    "            min_eval = None\n",
    "            for i in range(k):\n",
    "                fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "                if min_eval is None or fold_eval < min_eval:\n",
    "                    min_eval = fold_eval\n",
    "                    best_fold = i\n",
    "            y_counts_per_fold[best_fold] += y_counts\n",
    "            groups_per_fold[best_fold].add(g)\n",
    "\n",
    "        all_groups = set(groups)\n",
    "        for i in range(k):\n",
    "            train_groups = all_groups - groups_per_fold[i]\n",
    "            test_groups = groups_per_fold[i]\n",
    "\n",
    "            train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "            test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "            yield train_indices, test_indices\n",
    "            \n",
    "            \n",
    "def build_cv_spliter(\n",
    "    X,\n",
    "    y,\n",
    "    group=None,\n",
    "    strategy='stratified',\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_seed=42,\n",
    "    return_indices=False,\n",
    "):\n",
    "    if strategy == 'kfold':\n",
    "        kf = KFold(n_splits=n_splits, random_state=random_seed, shuffle=shuffle)\n",
    "        cv = kf.split(X)\n",
    "    elif strategy == 'stratified':\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=random_seed, shuffle=shuffle)\n",
    "        cv = kf.split(X, y)\n",
    "    elif strategy == 'group':\n",
    "        kf = MyGroupKFold(n_splits=n_splits, random_state=random_seed, shuffle=shuffle)\n",
    "        cv = kf.split(X, y, group)\n",
    "    elif strategy == 'stratified-group':\n",
    "        kf = StratifiedGroupKFold(n_splits=n_splits, random_state=random_seed, shuffle=shuffle)\n",
    "        cv = kf.split(X, y, group)\n",
    "    else:\n",
    "        raise NotImplementedError(f'strategy {strategy} not implemented.')\n",
    "    \n",
    "    if not return_indices:\n",
    "        cv_spliter = []\n",
    "        for dev_idx, val_idx in cv:\n",
    "            cv_spliter.append([dev_idx, val_idx])\n",
    "        return cv_spliter\n",
    "    else:\n",
    "        fold_indices = np.zeros(len(X), dtype=np.int64)\n",
    "        for fold, (_, val_idx) in enumerate(cv):\n",
    "            fold_indices[val_idx] = int(fold)\n",
    "        return fold_indices\n",
    "    \n",
    "    \n",
    "def cvt_cv2indices(cv):\n",
    "    # get total length\n",
    "    length = 0\n",
    "    for _, val_idx in cv:\n",
    "        length += len(val_idx)\n",
    "    # cvt cv -> indices\n",
    "    fold_indices = np.zeros(length, dtype=np.int64)\n",
    "    for fold, (_, val_idx) in enumerate(cv):\n",
    "        fold_indices[val_idx] = int(fold)\n",
    "    return fold_indices\n",
    "\n",
    "\n",
    "def cvt_indices2cv(indices):\n",
    "    cv_spliter = []\n",
    "    for fold in set(indices):\n",
    "        dev_idx = np.where(indices != fold)[0]\n",
    "        val_idx = np.where(indices == fold)[0]\n",
    "        cv_spliter.append([dev_idx, val_idx])\n",
    "    return cv_spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_high_corr_columns(df, threshold=0.99, verbose=1):\n",
    "    df_corr = abs(df.corr())\n",
    "    delete_columns = []\n",
    "    \n",
    "    # diagonal values filled by zero\n",
    "    for i in range(0, len(df_corr.columns)):\n",
    "        df_corr.iloc[i, i] = 0\n",
    "    \n",
    "    # loop as removing high-correlated columns in df_corr\n",
    "    while True:\n",
    "        df_max_column_value = df_corr.max()\n",
    "        max_corr = df_max_column_value.max()\n",
    "        query_column = df_max_column_value.idxmax()\n",
    "        target_column = df_corr[query_column].idxmax()\n",
    "        \n",
    "        if max_corr < threshold:\n",
    "            break\n",
    "        else:\n",
    "            # drop feature which is highly correlated with others \n",
    "            if sum(df_corr[query_column]) <= sum(df_corr[target_column]):\n",
    "                delete_column = target_column\n",
    "                saved_column = query_column\n",
    "            else:\n",
    "                delete_column = query_column\n",
    "                saved_column = target_column\n",
    "            \n",
    "            df_corr.drop([delete_column], axis=0, inplace=True)\n",
    "            df_corr.drop([delete_column], axis=1, inplace=True)\n",
    "            delete_columns.append(delete_column)\n",
    "            \n",
    "            if verbose > 0:\n",
    "                printl('{}: Drop: {} <- Query: {}, Corr: {:.5f}'.format(\n",
    "                    len(delete_columns), delete_column, saved_column, max_corr\n",
    "                ))\n",
    "\n",
    "    return delete_columns\n",
    "\n",
    "\n",
    "def _get_lgb_fimp(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    shuffle,\n",
    "    seed=42,\n",
    "    categorical=[]\n",
    "):\n",
    "    # Shuffle target if required\n",
    "    y = y_train.copy()\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        y = y_train.copy().sample(frac=1.0)\n",
    "    \n",
    "    arg_categorical = categorical if len(categorical) > 0 else 'auto'\n",
    "    dtrain = lgb.Dataset(X_train[features],\n",
    "                         label=y.values,\n",
    "                         categorical_feature=arg_categorical)\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params, dtrain)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = features\n",
    "    imp_df['split'] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['gain'] = clf.feature_importance(importance_type='gain')\n",
    "    \n",
    "    return imp_df\n",
    "\n",
    "\n",
    "def null_importance_selection(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    seed=42,\n",
    "    categorical=[],\n",
    "    num_actual_run=1,\n",
    "    num_null_run=40,\n",
    "    eps=1e-10,\n",
    "    valid_percentile=75,\n",
    "):\n",
    "    actual_imp_df = pd.DataFrame()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for i in tqdm(range(num_actual_run)):\n",
    "        seed = np.random.randint(1000)\n",
    "        imp_df = _get_lgb_fimp(params,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               features,\n",
    "                               shuffle=False,\n",
    "                               seed=seed,\n",
    "                               categorical=categorical)\n",
    "        imp_df['run'] = i\n",
    "        actual_imp_df = pd.concat([actual_imp_df, imp_df], axis=0)\n",
    "    \n",
    "    null_imp_df = pd.DataFrame()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for i in tqdm(range(num_null_run)):\n",
    "        seed = np.random.randint(1000)\n",
    "        imp_df = _get_lgb_fimp(params,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               features,\n",
    "                               shuffle=True,\n",
    "                               seed=seed,\n",
    "                               categorical=categorical)\n",
    "        imp_df['run'] = i\n",
    "        null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    \n",
    "    feature_scores = []\n",
    "    \n",
    "    for _f in actual_imp_df['feature'].unique():\n",
    "        # importance gain of gain\n",
    "        act_fimp_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'split'].mean()\n",
    "        null_fimp_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'split'].values\n",
    "        split_score = np.log(eps + act_fimp_split / (1 + np.percentile(null_fimp_split, valid_percentile)))\n",
    "        \n",
    "        # importance gain of gain\n",
    "        act_fimp_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'gain'].mean()\n",
    "        null_fimp_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'gain'].values\n",
    "        gain_score = np.log(eps + act_fimp_gain / (1 + np.percentile(null_fimp_gain, valid_percentile)))\n",
    "\n",
    "        feature_scores.append((_f, split_score, gain_score))\n",
    "    \n",
    "    scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(\n",
    "    feature_importance_df,\n",
    "    feature_name='feature',\n",
    "    importance_name=['split', 'gain'],\n",
    "    top_k=50,\n",
    "    fig_width=16,\n",
    "    fig_height=8,\n",
    "    fontsize=14,\n",
    "):\n",
    "    if isinstance(importance_name, str):\n",
    "        importance_name = [importance_name]\n",
    "    \n",
    "    num_importance = len(importance_name)\n",
    "    plt.figure(figsize=(fig_width, fig_height*num_importance))\n",
    "    gs = gridspec.GridSpec(1, num_importance)\n",
    "    \n",
    "    def _fetch_best_features(df, fimp='gain'):\n",
    "        cols = (df[[feature_name, fimp]]\n",
    "                .groupby(feature_name)\n",
    "                .mean()\n",
    "                .sort_values(by=fimp, ascending=False)\n",
    "                .index\n",
    "                .values[:top_k])\n",
    "        return cols, df.loc[df[feature_name].isin(cols)]\n",
    "    \n",
    "    for i, fimp in enumerate(importance_name):\n",
    "        cols, best_features = _fetch_best_features(feature_importance_df, fimp)\n",
    "        ax = plt.subplot(gs[0, i])\n",
    "        sns.barplot(x=fimp, y=feature_name, data=best_features, order=cols, ax=ax)\n",
    "        title = f'Features {fimp} importance (averaged/folds)'\n",
    "        plt.title(title, fontweight='bold', fontsize=fontsize)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_lightgbm(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "    categorical=[],\n",
    "    verbose_eval=100,\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    printl(f\"k={n_splits} folds lightgbm running...\")\n",
    "    printl(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        arg_categorical = categorical if len(categorical) > 0 else 'auto'\n",
    "        dev_data = lgb.Dataset(X_train.loc[dev_idx, features],\n",
    "                               label=y_train[dev_idx],\n",
    "                               categorical_feature=arg_categorical)\n",
    "        val_data = lgb.Dataset(X_train.loc[val_idx, features],\n",
    "                               label=y_train[val_idx],\n",
    "                               categorical_feature=arg_categorical)\n",
    "        \n",
    "        clf = lgb.train(params, dev_data, valid_sets=[dev_data, val_data], verbose_eval=verbose_eval)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        oof[val_idx] = clf.predict(X_train.loc[val_idx, features], num_iteration=clf.best_iteration)\n",
    "        predictions += clf.predict(X_test[features], num_iteration=clf.best_iteration) / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        printl(msg)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = features\n",
    "        fold_importance_df['split'] = clf.feature_importance(importance_type='split')\n",
    "        fold_importance_df['gain'] = clf.feature_importance(importance_type='gain')\n",
    "        fold_importance_df['fold'] = fold\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    printl(msg)\n",
    "\n",
    "    return oof, predictions, feature_importance_df\n",
    "\n",
    "\n",
    "def advarsarial_validation_lightgbm(\n",
    "    params,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    features,\n",
    "    categorical=[],\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "):\n",
    "    X_train_adv = X_train.copy()\n",
    "    X_test_adv = X_test.copy()\n",
    "    \n",
    "    X_train_adv['test'] = 0\n",
    "    X_test_adv['test'] = 1\n",
    "    \n",
    "    X_train_adv = pd.concat([X_train_adv, X_test_adv], axis=0).reset_index(drop=True)\n",
    "    y_train_adv = X_train_adv['test']\n",
    "    X_train_adv = X_train_adv.drop('test', axis=1)\n",
    "    \n",
    "    printl(f'{X_train_adv.shape}, {y_train_adv.shape}, {len(features)}')\n",
    "    \n",
    "    cv = build_cv_spliter(X_train_adv,\n",
    "                          y_train_adv,\n",
    "                          strategy='stratified',\n",
    "                          n_splits=n_splits,\n",
    "                          shuffle=shuffle,\n",
    "                          random_seed=seed)\n",
    "    \n",
    "    adv_metrics = {'AUC': roc_auc_score}\n",
    "    _, adv, feature_importance_df = run_kfold_lightgbm(params,\n",
    "                                                       X_train_adv,\n",
    "                                                       y_train_adv,\n",
    "                                                       X_train,\n",
    "                                                       cv,\n",
    "                                                       features,\n",
    "                                                       adv_metrics,\n",
    "                                                       categorical=cat_features)\n",
    "    \n",
    "    return adv, feature_importance_df\n",
    "\n",
    "\n",
    "def run_kfold_catboost_clf(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "    categorical=[],\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    printl(f\"k={n_splits} folds catboost running...\")\n",
    "    printl(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        dev_data = cb.Pool(X_train.loc[dev_idx, features],\n",
    "                           label=y_train[dev_idx],\n",
    "                           cat_features=categorical)\n",
    "        val_data = cb.Pool(X_train.loc[val_idx, features],\n",
    "                           label=y_train[val_idx],\n",
    "                           cat_features=categorical)\n",
    "        \n",
    "        clf = cb.CatBoostClassifier(**params)\n",
    "        clf.fit(dev_data, eval_set=val_data)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        oof[val_idx] = clf.predict_proba(X_train.loc[val_idx, features])[:,1]\n",
    "        predictions += clf.predict_proba(X_test[features])[:,1] / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        printl(msg)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = features\n",
    "        fold_importance_df['gain'] = clf.get_feature_importance()\n",
    "        fold_importance_df['fold'] = fold\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    printl(msg)\n",
    "\n",
    "    return oof, predictions, feature_importance_df\n",
    "\n",
    "\n",
    "def run_kfold_catboost_regr(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "    categorical=[],\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    printl(f\"k={n_splits} folds catboost running...\")\n",
    "    printl(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        dev_data = cb.Pool(X_train.loc[dev_idx, features],\n",
    "                           label=y_train[dev_idx],\n",
    "                           cat_features=categorical)\n",
    "        val_data = cb.Pool(X_train.loc[val_idx, features],\n",
    "                           label=y_train[val_idx],\n",
    "                           cat_features=categorical)\n",
    "        \n",
    "        clf = cb.CatBoostRegressor(**params)\n",
    "        clf.fit(dev_data, eval_set=val_data)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        oof[val_idx] = clf.predict(X_train.loc[val_idx, features])\n",
    "        predictions += clf.predict(X_test[features]) / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        printl(msg)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = features\n",
    "        fold_importance_df['gain'] = clf.get_feature_importance()\n",
    "        fold_importance_df['fold'] = fold\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    printl(msg)\n",
    "\n",
    "    return oof, predictions, feature_importance_df\n",
    "\n",
    "\n",
    "def build_neuralnet(\n",
    "    recipe,\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    lr=1e-3,\n",
    "    monitor='val_loss',\n",
    "    es_patience=-1,\n",
    "    restore_best_weights=True,\n",
    "    lr_scheduler='none',\n",
    "    lr_factor=0.1,\n",
    "    lr_patience=5,\n",
    "    seed=42,\n",
    "    **_,\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "    model = keras.models.model_from_json(recipe)\n",
    "    \n",
    "    if loss == 'mse':\n",
    "        loss = keras.losses.mean_squared_error\n",
    "    elif loss == 'bce':\n",
    "        loss = keras.losses.binary_crossentropy\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(lr)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    callbacks = []\n",
    "    \n",
    "    if es_patience >= 0:\n",
    "        es = keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "                                           patience=es_patience,\n",
    "                                           restore_best_weights=restore_best_weights,\n",
    "                                           verbose=1)\n",
    "        callbacks.append(es)\n",
    "    \n",
    "    if lr_scheduler == 'none':\n",
    "        pass\n",
    "    elif lr_scheduler == 'reduce_on_plateau':\n",
    "        lr_sche = keras.callbacks.ReduceLROnPlateau(monitor=monitor,\n",
    "                                                    factor=lr_factor,\n",
    "                                                    patience=lr_patience,\n",
    "                                                    verbose=1)\n",
    "        callbacks.append(lr_sche)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return model, callbacks\n",
    "\n",
    "\n",
    "def train_neuralnet(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=None,\n",
    "):\n",
    "    model, callbacks = build_neuralnet(**params)\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              validation_data=validation_data,\n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=params['epochs'],\n",
    "              callbacks=callbacks)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_kfold_neuralnet(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    printl(f\"k={n_splits} folds neuralnet running...\")\n",
    "    printl(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        validation_data = [X_train.loc[val_idx, features], y_train[val_idx]]\n",
    "        model = train_neuralnet(params,\n",
    "                                X_train.loc[dev_idx, features],\n",
    "                                y_train[dev_idx],\n",
    "                                validation_data=validation_data)\n",
    "        \n",
    "        oof[val_idx] = model.predict(X_train.loc[val_idx, features].values)[:,0]\n",
    "        predictions += model.predict(X_test[features].values)[:,0] / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        printl(msg)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    printl(msg)\n",
    "\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_threshold_search(y_true, y_proba, linspace=100):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(linspace)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggs = {\n",
    "#     'uid': ['count'],\n",
    "#     'is_manual': ['sum', 'mean'],\n",
    "#     'elapsed_days_succeeded_created': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_created_premium': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_created': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_succeeded_premium': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_succeeded': ['mean', 'std', 'max', 'min'],\n",
    "#     'created_before_premium': ['sum', 'mean'],\n",
    "#     'created_after_premium': ['sum', 'mean'],\n",
    "#     'succeeded_before_premium': ['sum', 'mean'],\n",
    "#     'succeeded_before_premium': ['sum', 'mean'],\n",
    "# }\n",
    "# aggs.update({col: ['sum', 'mean'] for col in service_category_id_cols})\n",
    "\n",
    "# group_account_df = account_df.groupby(ID).agg(aggs)\n",
    "# group_account_df.columns = [f'{k}_{v.upper()}' for k, vs in aggs.items() for v in vs]\n",
    "# group_account_df = group_account_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggs = {\n",
    "#     'uid': ['count'],\n",
    "#     'access_count': ['sum', 'mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_accessed': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_accessed_premium': ['mean', 'std', 'max', 'min'],\n",
    "#     'accessed_before_premium': ['sum', 'mean'],\n",
    "#     'accessed_after_premium': ['sum', 'mean'],\n",
    "#     'channel_flag_url_type_count': ['mean', 'std', 'max', 'min']\n",
    "# }\n",
    "# aggs.update({col: ['sum', 'mean'] for col in channel_flag_cols})\n",
    "# aggs.update({col: ['sum', 'mean'] for col in url_type_cols})\n",
    "# aggs.update({col: ['sum', 'mean', 'std', 'max', 'min'] for col in channel_count_cols})\n",
    "# aggs.update({col: ['sum', 'mean', 'std', 'max', 'min'] for col in url_count_cols})\n",
    "\n",
    "# group_access_df = access_df.groupby(ID).agg(aggs)\n",
    "# group_access_df.columns = [f'{k}_{v.upper()}' for k, vs in aggs.items() for v in vs]\n",
    "# group_access_df = group_access_df.reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
