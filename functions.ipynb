{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce DataFrame Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "    from pandas.api.types import is_categorical_dtype\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_parallelize_run(df, func):\n",
    "    import multiprocessing\n",
    "    num_partitions, num_cores = psutil.cpu_count(), psutil.cpu_count()\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Related (Categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_concat(df, subject_cols, print_option=True):\n",
    "    na_col = list(df.columns[df.isna().any()])\n",
    "    for col in na_col:\n",
    "        df[col].fillna('', inplace=True)\n",
    "    temp_str = ''\n",
    "    for col in subject_cols:\n",
    "        temp_str += '_' + col\n",
    "    df[temp_str[1:]] = ''\n",
    "    for col in subject_cols:\n",
    "        df[temp_str[1:]] += df[col]\n",
    "    \n",
    "    if print_option:\n",
    "        print(\"Generated features: category_concat\")\n",
    "        print(f\"'{temp_str[1:]}',\")\n",
    "        print()\n",
    "    del na_col, temp_str, col; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encode (with smooth / without OOF consideration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_target_encode(df, test_df, target_col, cat_col, smooth=False, m=0, statistic=False, print_option=True):\n",
    "    # df[target_col] = np.log1p(df[target_col])\n",
    "    df_group = df.groupby(cat_col)[target_col]\n",
    "    group_mean = df_group.mean().astype(np.float16)\n",
    "    \n",
    "    # ===== smoothing =====\n",
    "    if smooth:\n",
    "        global_mean = df[target_col].mean()\n",
    "        group_count = df_group.count().astype(np.float16)\n",
    "        smoother = ((group_count * group_mean) + (m * global_mean)) / (group_count + m)\n",
    "        df[f'{cat_col}_smoothed_mean'] = df[f'{cat_col}'].map(smoother)\n",
    "        test_df[f'{cat_col}_smoothed_mean'] = test_df[f'{cat_col}'].map(smoother)\n",
    "        del global_mean, group_count, smoother; gc.collect()\n",
    "\n",
    "    # ===== no smoothing =====\n",
    "    elif smooth == False:\n",
    "        df[f'{cat_col}_mean'] = df[f'{cat_col}'].map(group_mean)\n",
    "        test_df[f'{cat_col}_mean'] = test_df[f'{cat_col}'].map(group_mean)\n",
    "        \n",
    "    # ===== more target statistic =====\n",
    "    if statistic:\n",
    "        group_min = df_group.min().astype(np.float16)\n",
    "        group_max = df_group.max().astype(np.float16)\n",
    "        group_std = df_group.std().astype(np.float16)\n",
    "        \n",
    "        df[f'{cat_col}_min'] = df[f'{cat_col}'].map(group_min)\n",
    "        test_df[f'{cat_col}_min'] = df[f'{cat_col}'].map(group_min)\n",
    "        \n",
    "        df[f'{cat_col}_max'] = df[f'{cat_col}'].map(group_max)\n",
    "        test_df[f'{cat_col}_max'] = df[f'{cat_col}'].map(group_max)\n",
    "        \n",
    "        df[f'{cat_col}_std'] = df[f'{cat_col}'].map(group_std)\n",
    "        test_df[f'{cat_col}_std'] = df[f'{cat_col}'].map(group_std)\n",
    "        \n",
    "        df[f'{cat_col}_range'] = df[f'{cat_col}_max'] - df[f'{cat_col}_min']\n",
    "        test_df[f'{cat_col}_range'] = df[f'{cat_col}_max'] - df[f'{cat_col}_min']\n",
    "        \n",
    "        group_Q1 = df_group.quantile(0.25).astype(np.float16)\n",
    "        group_Q2 = df_group.median().astype(np.float16)\n",
    "        group_Q3 = df_group.quantile(0.75).astype(np.float16)\n",
    "        \n",
    "        df[f'{cat_col}_Q1'] = df[f'{cat_col}'].map(group_Q1)\n",
    "        test_df[f'{cat_col}_Q1'] = test_df[f'{cat_col}'].map(group_Q1)\n",
    "        \n",
    "        df[f'{cat_col}_Q2'] = df[f'{cat_col}'].map(group_Q2)\n",
    "        train_df[f'{cat_col}_Q2'] = train_df[f'{cat_col}'].map(group_Q2)\n",
    "        \n",
    "        df[f'{cat_col}_Q3'] = df[f'{cat_col}'].map(group_Q3)\n",
    "        test_df[f'{cat_col}_Q3'] = test_df[f'{cat_col}'].map(group_Q3)\n",
    "        \n",
    "        df[f'{cat_col}_IQR'] = df[f'{cat_col}_Q1'] - df[f'{cat_col}_Q3']\n",
    "        test_df[f'{cat_col}_IQR'] = test_df[f'{cat_col}_Q1'] - test_df[f'{cat_col}_Q3']\n",
    "        \n",
    "        del group_min, group_max, group_std, group_Q1, group_Q2, group_Q3; gc.collect()\n",
    "        \n",
    "    if print_option:\n",
    "        print(\"Generated features: apply_target_encode\")\n",
    "        if smooth:\n",
    "            print(f\"'{cat_col}_smoothed_mean',\")\n",
    "            print()\n",
    "        elif smooth == False:\n",
    "            print(f\"'{cat_col}_mean',\")\n",
    "            print()\n",
    "        if statistic:\n",
    "            print(f\"'{cat_col}_min',\")\n",
    "            print(f\"'{cat_col}_max',\")\n",
    "            print(f\"'{cat_col}_std',\")\n",
    "            print(f\"'{cat_col}_range',\")\n",
    "            print(f\"'{cat_col}_Q1',\")\n",
    "            print(f\"'{cat_col}_Q2',\")\n",
    "            print(f\"'{cat_col}_Q3',\")\n",
    "            print(f\"'{cat_col}_IQR',\")\n",
    "            print()\n",
    "    del df_group, group_mean; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode (not ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_label_encode(df, subject_cols):\n",
    "    for str_col in subject_cols:\n",
    "        # ===== assumes Series of string =====\n",
    "        temp_dict = {value: i for i, value in enumerate(df[str_col].unique())}\n",
    "        df[str_col] = df[str_col].map(temp_dict)\n",
    "    del temp_dict, str_col; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_freq_encode(df, str_col, print_option=True):\n",
    "    temp_dict = {sample: df.loc[df[str_col]==sample].shape[0] for sample in df[str_col].unique()}\n",
    "    df[f'{str_col}_ratio'] = df[str_col].map(temp_dict) / df[str_col].shape[0]\n",
    "    \n",
    "    if print_option:\n",
    "        print(\"Generated features: apply_freq_encode\")\n",
    "        print(f\"'{str_col}_ratio'\")\n",
    "        print()\n",
    "    del temp_dict; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Related (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggs = {\n",
    "#     'uid': ['count'],\n",
    "#     'is_manual': ['sum', 'mean'],\n",
    "#     'elapsed_days_succeeded_created': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_created_premium': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_created': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_succeeded_premium': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_succeeded': ['mean', 'std', 'max', 'min'],\n",
    "#     'created_before_premium': ['sum', 'mean'],\n",
    "#     'created_after_premium': ['sum', 'mean'],\n",
    "#     'succeeded_before_premium': ['sum', 'mean'],\n",
    "#     'succeeded_before_premium': ['sum', 'mean'],\n",
    "# }\n",
    "# aggs.update({col: ['sum', 'mean'] for col in service_category_id_cols})\n",
    "\n",
    "# group_account_df = account_df.groupby(ID).agg(aggs)\n",
    "# group_account_df.columns = [f'{k}_{v.upper()}' for k, vs in aggs.items() for v in vs]\n",
    "# group_account_df = group_account_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclical Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cyclical(df, str_col):\n",
    "    # df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    # ===== assumes integer array =====\n",
    "    df -= df[str_col].min()\n",
    "    int_max = df[str_col].max()\n",
    "    df[f'{str_col}_sin'] = np.sin(2 * np.pi * df[str_col] / int_max)\n",
    "    df[f'{str_col}_cos'] = np.cos(2 * np.pi * df[str_col] / int_max)\n",
    "    del int_max; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mov_stat(df, str_col, list_windows, fix=False, print_option=True):\n",
    "\n",
    "    # ===== assumes timestamp is aligned =====\n",
    "    for win in list_windows:\n",
    "        rolled = df[str_col].rolling(window=win, min_periods=0)\n",
    "        mov_avg = rolled.mean().reset_index() #.astype(np.float16)\n",
    "        mov_max = rolled.max().reset_index() #.astype(np.float16)\n",
    "        mov_min = rolled.min().reset_index() #.astype(np.float16)\n",
    "        mov_std = rolled.std().reset_index() #.astype(np.float16)\n",
    "        # mov_Q1 = rolled.quantile(0.25).reset_index() #.astype(np.float16)\n",
    "        # mov_Q2 = rolled.quantile(0.5).reset_index() #.astype(np.float16)\n",
    "        # mov_Q3 = rolled.quantile(0.75).reset_index() #.astype(np.float16)\n",
    "\n",
    "        if fix:\n",
    "            formula = int((win/2) - win)\n",
    "            df[f'{str_col}_movavg_{win}'] = mov_avg[f'{str_col}'].shift(formula)\n",
    "            df[f'{str_col}_movmax_{win}'] = mov_max[f'{str_col}'].shift(formula)\n",
    "            df[f'{str_col}_movmin_{win}'] = mov_min[f'{str_col}'].shift(formula)\n",
    "            df[f'{str_col}_movstd_{win}'] = mov_std[f'{str_col}'].shift(formula)\n",
    "            # df[f'{str_col}_movQ1_{win}'] = mov_Q1[f'{str_col}'].shift(formula)\n",
    "            # df[f'{str_col}_movQ2_{win}'] = mov_Q2[f'{str_col}'].shift(formula)\n",
    "            # df[f'{str_col}_movQ3_{win}'] = mov_Q3[f'{str_col}'].shift(formula)\n",
    "            print()\n",
    "            del formula\n",
    "        else:\n",
    "            df[f'{str_col}_movavg_{win}'] = mov_avg[f'{str_col}']\n",
    "            df[f'{str_col}_movmax_{win}'] = mov_max[f'{str_col}']\n",
    "            df[f'{str_col}_movmin_{win}'] = mov_min[f'{str_col}']\n",
    "            df[f'{str_col}_movstd_{win}'] = mov_std[f'{str_col}']\n",
    "            # df[f'{str_col}_movQ1_{win}'] = mov_Q1[f'{str_col}']\n",
    "            # df[f'{str_col}_movQ2_{win}'] = mov_Q2[f'{str_col}']\n",
    "            # df[f'{str_col}_movQ3_{win}'] = mov_Q3[f'{str_col}']\n",
    "            print()\n",
    "        \n",
    "        if print_option:\n",
    "            print('Generated features: apply_mov_stat')\n",
    "            print(f\"'{str_col}_movavg_{win}',\")\n",
    "            print(f\"'{str_col}_movmax_{win}',\")\n",
    "            print(f\"'{str_col}_movmin_{win}',\")\n",
    "            print(f\"'{str_col}_movstd_{win}',\")\n",
    "            #print(f\"'{str_col}_movQ1_{win}',\")\n",
    "            #print(f\"'{str_col}_movQ2_{win}',\")\n",
    "            #print(f\"'{str_col}_movQ3_{win}',\")\n",
    "            print()\n",
    "            \n",
    "    del win, rolled, mov_avg, mov_max, mov_min, mov_std; gc.collect()\n",
    "    # del mov_Q1, mov_Q2, mov_Q3; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear (log1p) Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nonlinear(df, subject_cols):\n",
    "    for col in subject_cols:\n",
    "        temp_count = df[f'{col}'].isna().sum()\n",
    "        df[f'{col}'] = np.log1p(df[f'{col}'])\n",
    "        if df[f'{col}'].isna().sum() > temp_count:\n",
    "            print(f\"New nan in '{col}' via apply_nonlinear\")\n",
    "    del col, temp_count; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_shift_feature(df, subject_cols, list_shift, print_option=True):\n",
    "    for col in subject_cols:\n",
    "        for step in list_shift:\n",
    "            df[f'{col}_shift_{step}'] = df[col].shift(int(step))\n",
    "            \n",
    "    if print_option:\n",
    "        print('Generated features: apply_shift_feature')\n",
    "        for col in subject_cols:\n",
    "            for step in list_shift:\n",
    "                print(f\"'{col}_shift_{step}',\")\n",
    "        print()\n",
    "        \n",
    "    del col, step; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oneth Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_oneth_feature(df, str_col, print_option=True):\n",
    "    import math\n",
    "    modify = np.vectorize(math.modf)\n",
    "    oneth, tenth = modify(df[str_col] / 10)\n",
    "    df[f'{str_col}_oneth'] = oneth * 10\n",
    "    \n",
    "    if print_option:\n",
    "        print('Generated features: apply_oneth_feature')\n",
    "        print(f\"'{str_col}_oneth',\")\n",
    "        print()\n",
    "        \n",
    "    del tenth; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nan Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_isna_feature(df, subject_cols, print_option=True):\n",
    "    binary_isna = [col+\"_isnan\" for col in subject_cols]\n",
    "    df[binary_isna] = df[subject_cols].isna().astype(int)\n",
    "    \n",
    "    if print_option:\n",
    "        print('Generated features: apply_oneth_feature')\n",
    "        for col in binary_isna:\n",
    "            print(f\"'{col}',\")\n",
    "        print()\n",
    "        \n",
    "    del binary_isna; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row nan Count Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_row_nan(df, print_option=True):\n",
    "    df['row_nan'] = df.isna().sum(axis=1).astype(np.int8)\n",
    "    \n",
    "    if print_option:\n",
    "        print('Generated features: apply_row_nan')\n",
    "        print(\"'row_nan',\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruteforce Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruteforce_combination(df, subject_cols, choose=2, print_option=True):\n",
    "    from itertools import combinations\n",
    "    comb = combinations(subject_cols, choose)\n",
    "    for feat_1, feat_2 in comb:\n",
    "        df[f'{feat_1}_.+_{feat_2}'] = df[f'{feat_1}'] + df[f'{feat_1}']\n",
    "        df[f'{feat_1}_.-_{feat_2}'] = df[f'{feat_1}'] - df[f'{feat_1}']\n",
    "        df[f'{feat_1}_.*_{feat_2}'] = df[f'{feat_1}'] * df[f'{feat_1}']\n",
    "        df[f'{feat_1}_./_{feat_2}'] = df[f'{feat_1}'] / df[f'{feat_1}']\n",
    "            \n",
    "    if print_option:\n",
    "        print('Generated features: bruteforce_feature_combination')\n",
    "        for feat_1, feat_2 in comb:\n",
    "            print(f\"'{feat_1}_.+_{feat_2}',\")\n",
    "            print(f\"'{feat_1}_.-_{feat_2}',\")\n",
    "            print(f\"'{feat_1}_.*_{feat_2}',\")\n",
    "            print(f\"'{feat_1}_./_{feat_2}',\")\n",
    "        print()\n",
    "            \n",
    "    del comb, feat_1, feat_2; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clip(df, str_col, pct_lower, pct_upper):\n",
    "    LB, UB = np.percentile(df[str_col], [pct_lower, pct_upper])\n",
    "    df[str_col] = np.clip(df[str_col], LB, UB)\n",
    "    del LB, UB; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_interpolation(df, subject_cols, int_order, supp_median_fill=False):\n",
    "    lin = lambda var: var.interpolate(method='linear', limit_direction='both')\n",
    "    pol = lambda var: var.interpolate(method='polynomial', order=int_order, limit_direction='both')\n",
    "    \n",
    "    # ===== in ASHRAE, grouping was done via site_id =====\n",
    "    # linear = df.groupby(grouping_col).apply(lin)\n",
    "    # polyno = df.groupby(grouping_col).apply(pol)\n",
    "    \n",
    "    linear = df[subject_cols].apply(lin)\n",
    "    polyno = df[subject_cols].apply(pol)\n",
    "    df[subject_cols] = (linear[subject_cols] + polyno[subject_cols]) * 0.5\n",
    "    \n",
    "    # ===== if missing value remains: =====\n",
    "    if supp_median_fill:\n",
    "        #[col for col in cols if temp[col].isna().sum() > 0]\n",
    "        for col in subject_cols:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "            del col\n",
    "    del lin, pol, linear, polyno; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Validation with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advarsarial_validation_lightgbm(\n",
    "    params,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    features,\n",
    "    categorical=[],\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "):\n",
    "    X_train_adv = X_train.copy()\n",
    "    X_test_adv = X_test.copy()\n",
    "    \n",
    "    X_train_adv['test'] = 0\n",
    "    X_test_adv['test'] = 1\n",
    "    \n",
    "    X_train_adv = pd.concat([X_train_adv, X_test_adv], axis=0).reset_index(drop=True)\n",
    "    y_train_adv = X_train_adv['test']\n",
    "    X_train_adv = X_train_adv.drop('test', axis=1)\n",
    "    \n",
    "    printl(f'{X_train_adv.shape}, {y_train_adv.shape}, {len(features)}')\n",
    "    \n",
    "    cv = build_cv_spliter(X_train_adv,\n",
    "                          y_train_adv,\n",
    "                          strategy='stratified',\n",
    "                          n_splits=n_splits,\n",
    "                          shuffle=shuffle,\n",
    "                          random_seed=seed)\n",
    "    \n",
    "    adv_metrics = {'AUC': roc_auc_score}\n",
    "    _, adv, feature_importance_df = run_kfold_lightgbm(params,\n",
    "                                                       X_train_adv,\n",
    "                                                       y_train_adv,\n",
    "                                                       X_train,\n",
    "                                                       cv,\n",
    "                                                       features,\n",
    "                                                       adv_metrics,\n",
    "                                                       categorical=cat_features)\n",
    "    \n",
    "    return adv, feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM (Binary Classification: max auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train_auc(train_set, valid_set, metric,\n",
    "                  lr, nl, md, bf, ff, mcw, mdil, l1, l2):\n",
    "    params = {'objective': 'binary',\n",
    "              'metric': metric,\n",
    "              'boosting': 'gbdt',\n",
    "              'seed': 8982,\n",
    "              'learning_rate': lr,\n",
    "              'num_leaves': int(nl),\n",
    "              'max_depth': int(md),\n",
    "              'bagging_freq': int(5),\n",
    "              'bagging_fraction': bf,\n",
    "              'feature_fraction': ff,\n",
    "              'min_child_weight': mcw,   \n",
    "              'min_data_in_leaf': int(mdil),\n",
    "              'lambda_l1': l1,\n",
    "              'lambda_l2': l2}\n",
    "\n",
    "    # ===== train_set: expects tuple (X_train, y_train) =====\n",
    "    X_train, y_train = train_set\n",
    "    # ===== valid_set: expects tuple (X_valid, y_valid) =====\n",
    "    X_valid, y_valid = valid_set\n",
    "\n",
    "    # ===== define data in lgb terms =====\n",
    "    d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=category_cols)\n",
    "    d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=category_cols)\n",
    "    watchlist = [d_train, d_valid]\n",
    "\n",
    "    learning_curve = {}\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      valid_sets=watchlist,\n",
    "                      num_boost_round=600,\n",
    "                      evals_result=learning_curve,\n",
    "                      verbose_eval=200,\n",
    "                      early_stopping_rounds=20)\n",
    "    \n",
    "    best_score = {f'train_{metric}': model.best_score['training'][f'{metric}'],\n",
    "                  f'valid_{metric}': model.best_score['valid_1'][f'{metric}']}\n",
    "    return model, best_score, learning_curve\n",
    "\n",
    "def lgb_skfold(X_train, y_train,\n",
    "              learning_rate=0.05, num_leaves=31, max_depth=-1,\n",
    "              bagging_fraction=0.9, feature_fraction=0.9,\n",
    "              min_child_weight=1e-3, min_data_in_leaf=20,\n",
    "              lambda_l1=0.0, lambda_l2=0.0,\n",
    "              bayes_opt=True):\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=8982)\n",
    "    metric = 'auc'\n",
    "    #cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "    #print(cat_features)\n",
    "    \n",
    "    oofs = np.zeros(X_train.shape[0],)\n",
    "    models = []; learning_curves = []; valid_score = []; # best_scores = []\n",
    "    for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "        train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n",
    "        valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n",
    "\n",
    "        model, best_score, learning_curve = lgb_train_auc(train_set=train_data,\n",
    "                                                          valid_set=valid_data,\n",
    "                                                          metric=metric,\n",
    "                                                          lr=learning_rate,\n",
    "                                                          nl=num_leaves,\n",
    "                                                          md=max_depth,\n",
    "                                                          bf=bagging_fraction,\n",
    "                                                          ff=feature_fraction,\n",
    "                                                          mcw=min_child_weight,\n",
    "                                                          mdil=min_data_in_leaf,\n",
    "                                                          l1=lambda_l1,\n",
    "                                                          l2=lambda_l2)\n",
    "        oofs[valid_idx] = model.predict(X_train.iloc[valid_idx,:], num_iteration=model.best_iteration)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_score.append(best_score[f'valid_{metric}'])\n",
    "        # best_scores.append(best_score)\n",
    "        gc.collect()\n",
    "    \n",
    "    valid_score_avg = np.mean(valid_score)\n",
    "    valid_score_std = np.std(valid_score)\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return valid_avg_score\n",
    "    else:\n",
    "        return valid_score_avg, valid_score_std, models, oofs #best_scores, learning_curves\n",
    "\n",
    "def lgb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (20, 500), \n",
    "              'bagging_fraction' : (0.1, 1),\n",
    "              'feature_fraction' : (0.1, 1),\n",
    "              'min_child_weight': (0.001, 0.99),   \n",
    "              'min_data_in_leaf': (3, 200),\n",
    "              'max_depth': (-1, 100),\n",
    "              'lambda_l1': (0.1, 300), \n",
    "              'lambda_l2': (0.1, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=lgb_skfold, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM (Regression: min rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train(train_set, valid_set, metric,\n",
    "              lr, nl, md, bf, ff, mcw, mdil, l1, l2):\n",
    "    params = {'objective': 'regression',\n",
    "              'metric': metric,\n",
    "              'boosting': 'gbdt',\n",
    "              'seed': 8982,\n",
    "              'learning_rate': lr,\n",
    "              'num_leaves': int(nl),\n",
    "              'max_depth': int(md),\n",
    "              'bagging_freq': int(5),\n",
    "              'bagging_fraction': bf,\n",
    "              'feature_fraction': ff,\n",
    "              'min_child_weight': mcw,   \n",
    "              'min_data_in_leaf': int(mdil),\n",
    "              'lambda_l1': l1,\n",
    "              'lambda_l2': l2}\n",
    "\n",
    "    # ===== train_set: expects tuple (X_train, y_train) =====\n",
    "    X_train, y_train = train_set\n",
    "    # ===== valid_set: expects tuple (X_valid, y_valid) =====\n",
    "    X_valid, y_valid = valid_set\n",
    "\n",
    "    # ===== define data in lgb terms =====\n",
    "    d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=category_cols)\n",
    "    d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=category_cols)\n",
    "    watchlist = [d_train, d_valid]\n",
    "\n",
    "    learning_curve = {}\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      valid_sets=watchlist,\n",
    "                      num_boost_round=600,\n",
    "                      evals_result=learning_curve,\n",
    "                      verbose_eval=200,\n",
    "                      early_stopping_rounds=20)\n",
    "    \n",
    "    best_score = {f'train_{metric}': model.best_score['training'][f'{metric}'],\n",
    "                  f'valid_{metric}': model.best_score['valid_1'][f'{metric}']}\n",
    "    return model, best_score, learning_curve\n",
    "\n",
    "def lgb_kfold(X_train, y_train,\n",
    "              learning_rate=0.05, num_leaves=31, max_depth=-1,\n",
    "              bagging_fraction=0.9, feature_fraction=0.9,\n",
    "              min_child_weight=1e-3, min_data_in_leaf=20,\n",
    "              lambda_l1=0.0, lambda_l2=0.0,\n",
    "              bayes_opt=True):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=8982)\n",
    "    metric = 'rmse'\n",
    "    #cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "    #print(cat_features)\n",
    "    \n",
    "    oofs = np.zeros(X_train.shape[0],)\n",
    "    models = []; learning_curves = []; valid_score = []; # best_scores = []\n",
    "    for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "        train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n",
    "        valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n",
    "\n",
    "        model, best_score, learning_curve = lgb_train(train_set=train_data,\n",
    "                                                      valid_set=valid_data,\n",
    "                                                      metric=metric,\n",
    "                                                      lr=learning_rate,\n",
    "                                                      nl=num_leaves,\n",
    "                                                      md=max_depth,\n",
    "                                                      bf=bagging_fraction,\n",
    "                                                      ff=feature_fraction,\n",
    "                                                      mcw=min_child_weight,\n",
    "                                                      mdil=min_data_in_leaf,\n",
    "                                                      l1=lambda_l1,\n",
    "                                                      l2=lambda_l2)\n",
    "        oofs[valid_idx] = model.predict(X_train.iloc[valid_idx,:], num_iteration=model.best_iteration)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_score.append(best_score[f'valid_{metric}'])\n",
    "        # best_scores.append(best_score)\n",
    "        gc.collect()\n",
    "    \n",
    "    valid_score_avg = np.mean(valid_score)\n",
    "    valid_score_std = np.std(valid_score)\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return -valid_avg_score\n",
    "    else:\n",
    "        return valid_score_avg, valid_score_std, models, oofs #best_scores, learning_curves\n",
    "    \n",
    "def lgb_reg_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (20, 500), \n",
    "              'bagging_fraction' : (0.1, 1),\n",
    "              'feature_fraction' : (0.1, 1),\n",
    "              'min_child_weight': (0.001, 0.99),   \n",
    "              'min_data_in_leaf': (3, 200),\n",
    "              'max_depth': (-1, 100),\n",
    "              'lambda_l1': (0.1, 300), \n",
    "              'lambda_l2': (0.1, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=lgb_kfold, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', -optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = -optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(X_test, models):\n",
    "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'Predicting with {i}-th model')\n",
    "        y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_pred_total += y_pred_test\n",
    "    y_test_pred_total /= len(models)\n",
    "    return y_test_pred_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_kfold_clf(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "    categorical=[],\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    printl(f\"k={n_splits} folds catboost running...\")\n",
    "    printl(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        dev_data = cb.Pool(X_train.loc[dev_idx, features],\n",
    "                           label=y_train[dev_idx],\n",
    "                           cat_features=categorical)\n",
    "        val_data = cb.Pool(X_train.loc[val_idx, features],\n",
    "                           label=y_train[val_idx],\n",
    "                           cat_features=categorical)\n",
    "        \n",
    "        clf = cb.CatBoostClassifier(**params)\n",
    "        clf.fit(dev_data, eval_set=val_data)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        oof[val_idx] = clf.predict_proba(X_train.loc[val_idx, features])[:,1]\n",
    "        predictions += clf.predict_proba(X_test[features])[:,1] / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        printl(msg)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = features\n",
    "        fold_importance_df['gain'] = clf.get_feature_importance()\n",
    "        fold_importance_df['fold'] = fold\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    printl(msg)\n",
    "\n",
    "    return oof, predictions, feature_importance_df\n",
    "\n",
    "\n",
    "def cb_kfold_reg(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "    categorical=[],\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    printl(f\"k={n_splits} folds catboost running...\")\n",
    "    printl(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        dev_data = cb.Pool(X_train.loc[dev_idx, features],\n",
    "                           label=y_train[dev_idx],\n",
    "                           cat_features=categorical)\n",
    "        val_data = cb.Pool(X_train.loc[val_idx, features],\n",
    "                           label=y_train[val_idx],\n",
    "                           cat_features=categorical)\n",
    "        \n",
    "        clf = cb.CatBoostRegressor(**params)\n",
    "        clf.fit(dev_data, eval_set=val_data)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        oof[val_idx] = clf.predict(X_train.loc[val_idx, features])\n",
    "        predictions += clf.predict(X_test[features]) / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        printl(msg)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = features\n",
    "        fold_importance_df['gain'] = clf.get_feature_importance()\n",
    "        fold_importance_df['fold'] = fold\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    printl(msg)\n",
    "\n",
    "    return oof, predictions, feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neuralnet(\n",
    "    recipe,\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    lr=1e-3,\n",
    "    monitor='val_loss',\n",
    "    es_patience=-1,\n",
    "    restore_best_weights=True,\n",
    "    lr_scheduler='none',\n",
    "    lr_factor=0.1,\n",
    "    lr_patience=5,\n",
    "    seed=42,\n",
    "    **_,\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "    model = keras.models.model_from_json(recipe)\n",
    "    \n",
    "    if loss == 'mse':\n",
    "        loss = keras.losses.mean_squared_error\n",
    "    elif loss == 'bce':\n",
    "        loss = keras.losses.binary_crossentropy\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(lr)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    callbacks = []\n",
    "    \n",
    "    if es_patience >= 0:\n",
    "        es = keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "                                           patience=es_patience,\n",
    "                                           restore_best_weights=restore_best_weights,\n",
    "                                           verbose=1)\n",
    "        callbacks.append(es)\n",
    "    \n",
    "    if lr_scheduler == 'none':\n",
    "        pass\n",
    "    elif lr_scheduler == 'reduce_on_plateau':\n",
    "        lr_sche = keras.callbacks.ReduceLROnPlateau(monitor=monitor,\n",
    "                                                    factor=lr_factor,\n",
    "                                                    patience=lr_patience,\n",
    "                                                    verbose=1)\n",
    "        callbacks.append(lr_sche)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return model, callbacks\n",
    "\n",
    "\n",
    "def train_neuralnet(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=None,\n",
    "):\n",
    "    model, callbacks = build_neuralnet(**params)\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              validation_data=validation_data,\n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=params['epochs'],\n",
    "              callbacks=callbacks)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_kfold_neuralnet(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    printl(f\"k={n_splits} folds neuralnet running...\")\n",
    "    printl(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        validation_data = [X_train.loc[val_idx, features], y_train[val_idx]]\n",
    "        model = train_neuralnet(params,\n",
    "                                X_train.loc[dev_idx, features],\n",
    "                                y_train[dev_idx],\n",
    "                                validation_data=validation_data)\n",
    "        \n",
    "        oof[val_idx] = model.predict(X_train.loc[val_idx, features].values)[:,0]\n",
    "        predictions += model.predict(X_test[features].values)[:,0] / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        printl(msg)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    printl(msg)\n",
    "\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(\n",
    "    feature_importance_df,\n",
    "    feature_name='feature',\n",
    "    importance_name=['split', 'gain'],\n",
    "    top_k=50,\n",
    "    fig_width=16,\n",
    "    fig_height=8,\n",
    "    fontsize=14,\n",
    "):\n",
    "    if isinstance(importance_name, str):\n",
    "        importance_name = [importance_name]\n",
    "    \n",
    "    num_importance = len(importance_name)\n",
    "    plt.figure(figsize=(fig_width, fig_height*num_importance))\n",
    "    gs = gridspec.GridSpec(1, num_importance)\n",
    "    \n",
    "    def _fetch_best_features(df, fimp='gain'):\n",
    "        cols = (df[[feature_name, fimp]]\n",
    "                .groupby(feature_name)\n",
    "                .mean()\n",
    "                .sort_values(by=fimp, ascending=False)\n",
    "                .index\n",
    "                .values[:top_k])\n",
    "        return cols, df.loc[df[feature_name].isin(cols)]\n",
    "    \n",
    "    for i, fimp in enumerate(importance_name):\n",
    "        cols, best_features = _fetch_best_features(feature_importance_df, fimp)\n",
    "        ax = plt.subplot(gs[0, i])\n",
    "        sns.barplot(x=fimp, y=feature_name, data=best_features, order=cols, ax=ax)\n",
    "        title = f'Features {fimp} importance (averaged/folds)'\n",
    "        plt.title(title, fontweight='bold', fontsize=fontsize)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Feature Elimination by LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iterative_CV:\n",
    "    def __init__(self, X_train_full, y_train, eval_cols, metric):\n",
    "        self.X_train_full = X_train_full\n",
    "        self.y_train = y_train\n",
    "        self.eval_cols = eval_cols\n",
    "        self.metric = metric\n",
    "        \n",
    "    def iter_cv_elim():\n",
    "        excl_improve = []; excl_worse = []\n",
    "        if self.metric == 'rmse':\n",
    "            init_valid_avg_score = -1 * lgb_kfold(self.X_train_full, self.y_train, bayes_opt=True)\n",
    "            print(f'[Iter_Feature_Elim] Current best score is {init_valid_avg_score}')\n",
    "            for cols in tqdm(self.eval_cols):\n",
    "                temp_cols = list(set(self.X_train_full.columns) - {col})\n",
    "                X_train = self.X_train_full[temp_cols]\n",
    "                new_valid_avg_score = -1 * lgb_kfold(X_train, self.y_train, bayes_opt=True)\n",
    "                degree = new_valid_avg_score - init_valid_avg_score\n",
    "                if degree < 0:\n",
    "                    pct = 100 * (-1 * degree / init_valid_avg_score)\n",
    "                    excl_improve.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion improved (lowered) avg CV by {pct}pct.\")\n",
    "                else:\n",
    "                    pct = 100 * (degree / init_valid_avg_score)\n",
    "                    excl_worse.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion worsened (raised) avg CV by {pct}pct.\")\n",
    "        elif self.metric == 'auc':\n",
    "            init_valid_avg_score = lgb_skfold(self.X_train_full, self.y_train, bayes_opt=True)\n",
    "            print(f'[Iter_Feature_Elim] Current best score is {init_valid_avg_score}')\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                temp_cols = list(set(self.X_train_full.columns) - {col})\n",
    "                X_train = self.X_train_full[temp_cols]\n",
    "                new_valid_avg_score = lgb_skfold(X_train, self.y_train, bayes_opt=True)\n",
    "                degree = new_valid_avg_score - init_valid_avg_score\n",
    "                if degree > 0:\n",
    "                    pct = 100 * (degree / init_valid_avg_score)\n",
    "                    excl_improve.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion improved (raised) avg CV by {pct}pct.\")\n",
    "                else:\n",
    "                    pct = 100 * (-1 * degree / init_valid_avg_score)\n",
    "                    excl_worse.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion worsened (lowered) avg CV by {pct}pct.\")\n",
    "\n",
    "        excl_improve.sort(key=lambda lst: lst[1])\n",
    "        excl_worse.sort(key=lambda lst: lst[1])\n",
    "        del init_valid_avg_score, cols, temp_cols, X_train, new_valid_avg_score, degree, pct\n",
    "        gc.collect()\n",
    "        return excl_improve, excl_worse\n",
    "    \n",
    "    def iter_cv_rank():\n",
    "        impt = []\n",
    "        if self.metric == 'rmse':\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                X_train = self.X_train_full[col]\n",
    "                assert X_train.shape[1] == 1\n",
    "                print(f\"[Iter_Feature_Rank] '{col}', evaluation ongoing.\")\n",
    "                valid_avg_score = -1 * lgb_kfold(X_train, self.y_train, bayes_opt=True)\n",
    "                impt.append([col, valid_avg_score])\n",
    "        elif self.metric == 'auc':\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                X_train = self.X_train_full[col]\n",
    "                assert X_train.shape[1] == 1\n",
    "                print(f\"[Iter_Feature_Rank] '{col}', evaluation ongoing.\")\n",
    "                valid_avg_score = lgb_skfold(X_train, self.y_train, bayes_opt=True)\n",
    "                impt.append([col, valid_avg_score])\n",
    "        impt.sort(key=lambda lst: lst[1])\n",
    "        del col, X_train, valid_avg_score\n",
    "        gc.collect()\n",
    "        return impt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Importance Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lgb_fimp(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    shuffle,\n",
    "    seed=42,\n",
    "    categorical=[]\n",
    "):\n",
    "    # Shuffle target if required\n",
    "    y = y_train.copy()\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        y = y_train.copy().sample(frac=1.0)\n",
    "    \n",
    "    arg_categorical = categorical if len(categorical) > 0 else 'auto'\n",
    "    dtrain = lgb.Dataset(X_train[features],\n",
    "                         label=y.values,\n",
    "                         categorical_feature=arg_categorical)\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params, dtrain)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = features\n",
    "    imp_df['split'] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['gain'] = clf.feature_importance(importance_type='gain')\n",
    "    \n",
    "    return imp_df\n",
    "\n",
    "\n",
    "def null_importance_selection(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    seed=42,\n",
    "    categorical=[],\n",
    "    num_actual_run=1,\n",
    "    num_null_run=40,\n",
    "    eps=1e-10,\n",
    "    valid_percentile=75,\n",
    "):\n",
    "    actual_imp_df = pd.DataFrame()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for i in tqdm(range(num_actual_run)):\n",
    "        seed = np.random.randint(1000)\n",
    "        imp_df = _get_lgb_fimp(params,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               features,\n",
    "                               shuffle=False,\n",
    "                               seed=seed,\n",
    "                               categorical=categorical)\n",
    "        imp_df['run'] = i\n",
    "        actual_imp_df = pd.concat([actual_imp_df, imp_df], axis=0)\n",
    "    \n",
    "    null_imp_df = pd.DataFrame()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for i in tqdm(range(num_null_run)):\n",
    "        seed = np.random.randint(1000)\n",
    "        imp_df = _get_lgb_fimp(params,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               features,\n",
    "                               shuffle=True,\n",
    "                               seed=seed,\n",
    "                               categorical=categorical)\n",
    "        imp_df['run'] = i\n",
    "        null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    \n",
    "    feature_scores = []\n",
    "    \n",
    "    for _f in actual_imp_df['feature'].unique():\n",
    "        # importance gain of gain\n",
    "        act_fimp_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'split'].mean()\n",
    "        null_fimp_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'split'].values\n",
    "        split_score = np.log(eps + act_fimp_split / (1 + np.percentile(null_fimp_split, valid_percentile)))\n",
    "        \n",
    "        # importance gain of gain\n",
    "        act_fimp_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'gain'].mean()\n",
    "        null_fimp_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'gain'].values\n",
    "        gain_score = np.log(eps + act_fimp_gain / (1 + np.percentile(null_fimp_gain, valid_percentile)))\n",
    "\n",
    "        feature_scores.append((_f, split_score, gain_score))\n",
    "    \n",
    "    scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_high_corr_columns(df, threshold=0.99, verbose=True):\n",
    "    df_corr = abs(df.corr())\n",
    "    delete_columns = []\n",
    "    \n",
    "    # diagonal values filled by zero\n",
    "    for i in range(0, len(df_corr.columns)):\n",
    "        df_corr.iloc[i, i] = 0\n",
    "    \n",
    "    # loop as removing high-correlated columns in df_corr\n",
    "    while True:\n",
    "        df_max_column_value = df_corr.max()\n",
    "        max_corr = df_max_column_value.max()\n",
    "        query_column = df_max_column_value.idxmax()\n",
    "        target_column = df_corr[query_column].idxmax()\n",
    "        \n",
    "        if max_corr < threshold:\n",
    "            break\n",
    "        else:\n",
    "            # drop feature which is highly correlated with others \n",
    "            if sum(df_corr[query_column]) <= sum(df_corr[target_column]):\n",
    "                delete_column = target_column\n",
    "                saved_column = query_column\n",
    "            else:\n",
    "                delete_column = query_column\n",
    "                saved_column = target_column\n",
    "            \n",
    "            df_corr.drop([delete_column], axis=0, inplace=True)\n",
    "            df_corr.drop([delete_column], axis=1, inplace=True)\n",
    "            delete_columns.append(delete_column)\n",
    "            \n",
    "            if verbose:\n",
    "                printl('{}: Drop: {} <- Query: {}, Corr: {:.5f}'.format(\n",
    "                    len(delete_columns), delete_column, saved_column, max_corr\n",
    "                ))\n",
    "\n",
    "    return delete_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
