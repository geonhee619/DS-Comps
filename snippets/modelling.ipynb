{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGroupKFold:\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        groups = pd.Series(groups)\n",
    "        unique_groups = np.unique(groups)\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n",
    "        for tr_group_idx, va_group_idx in kf.split(unique_groups):\n",
    "            tr_groups, va_groups = unique_groups[tr_group_idx], unique_groups[va_group_idx]\n",
    "            tr_indices = groups[groups.isin(tr_groups)].index.to_list()\n",
    "            va_indices = groups[groups.isin(va_groups)].index.to_list()\n",
    "            yield tr_indices, va_indices\n",
    "            \n",
    "class StratifiedGroupKFold:\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    # Implementation based on this kaggle kernel:\n",
    "    #    https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        k = self.n_splits\n",
    "        rnd = check_random_state(self.random_state)\n",
    "            \n",
    "        # labels_num: zero-origin number of label\n",
    "        # ex) unique = [0,1,2,3] -> labels_num = 4\n",
    "        labels_num = np.max(y) + 1\n",
    "        \n",
    "        # y_counts_per_group: in-group label distribution\n",
    "        # y_distr: whole label distribution\n",
    "        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "        y_distr = Counter()\n",
    "        for label, g in zip(y, groups):\n",
    "            y_counts_per_group[g][label] += 1\n",
    "            y_distr[label] += 1\n",
    "\n",
    "        # y_counts_per_fold: in-fold label distribution\n",
    "        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "        groups_per_fold = defaultdict(set)\n",
    "        \n",
    "        # return mean std of per label counts when y_counts is in fold\n",
    "        def eval_y_counts_per_fold(y_counts, fold):\n",
    "            y_counts_per_fold[fold] += y_counts\n",
    "            std_per_label = []\n",
    "            for label in range(labels_num):\n",
    "                label_std = np.std(\n",
    "                    [y_counts_per_fold[i][label] / y_distr[label] for i in range(k)]\n",
    "                )\n",
    "                std_per_label.append(label_std)\n",
    "            y_counts_per_fold[fold] -= y_counts\n",
    "            return np.mean(std_per_label)\n",
    "        \n",
    "        # list of [group, y_counts]\n",
    "        # if shuffle: fold changes in same np.std(y_counts)\n",
    "        # ascending groups by degree of label variance\n",
    "        groups_and_y_counts = list(y_counts_per_group.items())\n",
    "        if self.shuffle:\n",
    "            rnd.shuffle(groups_and_y_counts)\n",
    "        groups_and_y_counts = sorted(groups_and_y_counts, key=lambda x: -np.std(x[1]))\n",
    "\n",
    "        # set fold for each group such that label distirbution will be uniform\n",
    "        for g, y_counts in groups_and_y_counts:\n",
    "            best_fold = None\n",
    "            min_eval = None\n",
    "            for i in range(k):\n",
    "                fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "                if min_eval is None or fold_eval < min_eval:\n",
    "                    min_eval = fold_eval\n",
    "                    best_fold = i\n",
    "            y_counts_per_fold[best_fold] += y_counts\n",
    "            groups_per_fold[best_fold].add(g)\n",
    "\n",
    "        all_groups = set(groups)\n",
    "        for i in range(k):\n",
    "            train_groups = all_groups - groups_per_fold[i]\n",
    "            test_groups = groups_per_fold[i]\n",
    "\n",
    "            train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "            test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "            yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cv_spliter(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    strategy='stratified',\n",
    "    n_splits=5,\n",
    "    group=None,\n",
    "    shuffle=True,\n",
    "    seed=8982,\n",
    "    return_indices=False,\n",
    "):\n",
    "    if strategy == 'kfold':\n",
    "        kf = KFold(n_splits=n_splits, random_state=seed, shuffle=shuffle)\n",
    "        cv = kf.split(X_train)\n",
    "    elif strategy == 'stratified':\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=shuffle)\n",
    "        cv = kf.split(X_train, y_train)\n",
    "    elif strategy == 'group':\n",
    "        kf = MyGroupKFold(n_splits=n_splits, random_state=seed, shuffle=shuffle)\n",
    "        cv = kf.split(X_train, y_train, group)\n",
    "    elif strategy == 'stratified-group':\n",
    "        kf = StratifiedGroupKFold(n_splits=n_splits, random_state=seed, shuffle=shuffle)\n",
    "        cv = kf.split(X_train, y_train, group)\n",
    "    else:\n",
    "        raise NotImplementedError(f'strategy {strategy} not implemented.')\n",
    "\n",
    "    if not return_indices:\n",
    "        cv_spliter = []\n",
    "        for dev_idx, val_idx in cv:\n",
    "            cv_spliter.append([dev_idx, val_idx])\n",
    "        return cv_spliter\n",
    "    else:\n",
    "        fold_indices = np.zeros(len(X), dtype=np.int64)\n",
    "        for fold, (_, val_idx) in enumerate(cv):\n",
    "            fold_indices[val_idx] = int(fold)\n",
    "        return fold_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM (Binary Classification: max auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_kfold_clf(X_train, y_train, category_cols, split, bayes_opt=True,\n",
    "                  learning_rate=0.05, num_leaves=31, max_depth=-1,\n",
    "                  bagging_fraction=0.9, feature_fraction=0.9,\n",
    "                  min_child_weight=1e-3, min_data_in_leaf=20,\n",
    "                  lambda_l1=0.0, lambda_l2=0.0):\n",
    "    metric='auc'\n",
    "    params = {'objective': 'binary',\n",
    "              'metric': metric,\n",
    "              'boosting': 'gbdt',\n",
    "              'seed': 8982,\n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'bagging_freq': int(5),\n",
    "              'bagging_fraction': bagging_fraction,\n",
    "              'feature_fraction': feature_fraction,\n",
    "              'min_child_weight': min_child_weight,   \n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'lambda_l1': lambda_l1,\n",
    "              'lambda_l2': lambda_l2}\n",
    "              #'verbosity': int(-1)}\n",
    "             \n",
    "    #cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "    #print(cat_features)\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    models = []; learning_curves = []; best_scores = []; valid_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    print(f'========== LightGBM Classifier training on : {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        d_train = lgb.Dataset(X_train.iloc[train_idx,:], label=y_train[train_idx], categorical_feature=category_cols)\n",
    "        d_valid = lgb.Dataset(X_train.iloc[valid_idx,:], label=y_train[valid_idx], categorical_feature=category_cols)\n",
    "        \n",
    "        print(f'========== LightGBM Classifier training: {i+1}/{n_splits} fold ==========')\n",
    "        learning_curve = {}\n",
    "        model = lgb.train(params,\n",
    "                          train_set=d_train,\n",
    "                          valid_sets=[d_train, d_valid],\n",
    "                          num_boost_round=5000,\n",
    "                          early_stopping_rounds=20,\n",
    "                          evals_result=learning_curve,\n",
    "                          verbose_eval=200#False,\n",
    "                          )\n",
    "        best_score = {f'train_{metric}': model.best_score['training'][f'{metric}'],\n",
    "                      f'valid_{metric}': model.best_score['valid_1'][f'{metric}']}\n",
    "        print()\n",
    "        oofs[valid_idx] = model.predict(X_train.iloc[valid_idx,:], num_iteration=model.best_iteration)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_score.append(best_score[f'valid_{metric}'])\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = common_cols + category_cols\n",
    "        fold_importance_df['importance'] = model.feature_importance()\n",
    "        fold_importance_df['fold'] = i+1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        del d_train, d_valid, fold_importance_df\n",
    "        gc.collect()\n",
    "        \n",
    "    valid_std_score = np.std(valid_score)\n",
    "    valid_avg_score = np.mean(valid_score)\n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {valid_avg_score}')\n",
    "    print(f'CV STD: {metric} - {valid_std_score}')\n",
    "    print('====================')\n",
    "\n",
    "    if bayes_opt:\n",
    "        return valid_avg_score\n",
    "    else:\n",
    "        return oofs, models, feature_importance_df #best_scores, learning_curves\n",
    "\n",
    "def lgb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (20, 500), \n",
    "              #'max_depth': (-1, 250),\n",
    "              'bagging_fraction' : (0.1, 1),\n",
    "              'feature_fraction' : (0.1, 1),\n",
    "              'min_child_weight': (0.001, 0.99),   \n",
    "              'min_data_in_leaf': (3, 700),\n",
    "              'lambda_l1': (0.1, 300), \n",
    "              'lambda_l2': (0.1, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=lgb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_pred_clf(X_test, models, threshold):\n",
    "    y_test_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== LightGBM Predicting with {i+1}-th model ==========')\n",
    "        y_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_total += y_test\n",
    "    y_test_total /= len(models)\n",
    "    y_test = np.where(y_test_total > threshold, 1, 0)\n",
    "    gc.collect()\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM (Regression: min rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_kfold_reg(X_train, y_train, category_cols, split, bayes_opt=True,\n",
    "                  learning_rate=0.05, num_leaves=31, max_depth=-1,\n",
    "                  bagging_fraction=0.9, feature_fraction=0.9,\n",
    "                  min_child_weight=1e-3, min_data_in_leaf=20,\n",
    "                  lambda_l1=0.0, lambda_l2=0.0):\n",
    "    metric='rmse'\n",
    "    params = {'objective': 'regression',\n",
    "              'metric': metric,\n",
    "              'boosting': 'gbdt',\n",
    "              'seed': 8982,\n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'bagging_freq': int(5),\n",
    "              'bagging_fraction': bagging_fraction,\n",
    "              'feature_fraction': feature_fraction,\n",
    "              'min_child_weight': min_child_weight,   \n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'lambda_l1': lambda_l1,\n",
    "              'lambda_l2': lambda_l2}\n",
    "              #'verbosity': int(-1)}\n",
    "             \n",
    "    #cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "    #print(cat_features)\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    models = []; learning_curves = []; valid_scores = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    print(f'========== LightGBM Regressor training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        d_train = lgb.Dataset(X_train.iloc[train_idx,:], label=y_train[train_idx], categorical_feature=category_cols)\n",
    "        d_valid = lgb.Dataset(X_train.iloc[valid_idx,:], label=y_train[valid_idx], categorical_feature=category_cols)\n",
    "        \n",
    "        print(f'========== LightGBM Regressor training: {i+1}/{n_splits} fold ==========')\n",
    "        learning_curve = {}\n",
    "        model = lgb.train(params,\n",
    "                          train_set=d_train,\n",
    "                          valid_sets=[d_train, d_valid],\n",
    "                          num_boost_round=5000,\n",
    "                          early_stopping_rounds=20,\n",
    "                          evals_result=learning_curve,\n",
    "                          verbose_eval=200#False,\n",
    "                          )\n",
    "        print()\n",
    "        oofs[valid_idx] = model.predict(X_train.iloc[valid_idx,:], num_iteration=model.best_iteration)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_scores.append(model.best_score['valid_1'][f'{metric}'])\n",
    "          \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = common_cols + category_cols\n",
    "        fold_importance_df['importance'] = model.feature_importance()\n",
    "        fold_importance_df['fold'] = i+1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)  \n",
    "        \n",
    "        del d_train, d_valid, fold_importance_df\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    valid_std_score = np.std(valid_scores)\n",
    "    valid_avg_score = np.mean(valid_scores)\n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {valid_avg_score}')\n",
    "    print(f'CV STD: {metric} - {valid_std_score}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return -valid_avg_score\n",
    "    else:\n",
    "        return oofs, models, feature_importance_df #best_scores, learning_curves\n",
    "\n",
    "def lgb_reg_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (20, 500),\n",
    "              #'max_depth': (-1, 250),\n",
    "              'bagging_fraction' : (0.1, 1),\n",
    "              'feature_fraction' : (0.1, 1),\n",
    "              'min_child_weight': (0.001, 0.99),   \n",
    "              'min_data_in_leaf': (3, 700),\n",
    "              'lambda_l1': (0.1, 300), \n",
    "              'lambda_l2': (0.1, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=lgb_kfold_reg, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', -optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = -optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_pred_reg(X_test, models):\n",
    "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== LightGBM Predicting with {i+1}-th model ==========')\n",
    "        y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_pred_total += y_pred_test\n",
    "    y_test_pred_total /= len(models)\n",
    "    return y_test_pred_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost (Binary Classification: max AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7fdff811ac89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_kfold_clf(X_train, y_train, category_cols, split, bayes_opt=True,\n",
    "                learning_rate=0.03, num_leaves=31, max_depth=6,\n",
    "                subsample=0.8, bagging_temperature=1.0, colsample_bylevel=1.0,\n",
    "                min_data_in_leaf=1, l2_leaf_reg=3.0, random_strength=1.0):\n",
    "    loss = 'Logloss'\n",
    "    metric = 'AUC'\n",
    "    params = {'loss_function': loss,\n",
    "              'eval_metric': metric,\n",
    "              'boosting_type': 'Plain',\n",
    "              'random_seed': 8982,\n",
    "              'num_boost_round': 5000,\n",
    "              'early_stopping_rounds': 20,\n",
    "              'use_best_model': True,\n",
    "              # 'grow_policy': 'SymmetricTree','Depthwise','Lossguide',\n",
    "              'nan_mode': 'Max',\n",
    "              'od_type': 'Iter',\n",
    "              'verbose': 200,\n",
    "              \n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': subsample, #bf?\n",
    "              'bagging_temperature': bagging_temperature, #bf?\n",
    "              'colsample_bylevel': colsample_bylevel, #ff\n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'l2_leaf_reg': l2_leaf_reg,\n",
    "              'random_strength': random_strength}\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    models = []; learning_curves = []; valid_losses = []; valid_metrics = []\n",
    "    print(f'========== CatBoost Classifier training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        print(f'========== CatBoost Classifier training: {i+1}/{n_splits} ==========')\n",
    "        train_d = cb.Pool(data=X_train.loc[train_idx],\n",
    "                          label=y_train[train_idx],\n",
    "                          cat_features=category_cols)\n",
    "        valid_d = cb.Pool(data=X_train.loc[valid_idx],\n",
    "                          label=y_train[valid_idx],\n",
    "                          cat_features=category_cols)\n",
    "        \n",
    "        model = cb.CatBoostClassifier(**params)\n",
    "        model.fit(train_d, eval_set=valid_d)\n",
    "        \n",
    "        oofs[valid_idx] = model.predict_proba(X_train.loc[valid_idx])[:,1]\n",
    "        models.append(model)\n",
    "        valid_losses.append(model.best_score_['validation'][f'{loss}'])\n",
    "        valid_metrics.append(model.best_score_['validation'][f'{metric}'])\n",
    "          \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = common_cols + category_cols\n",
    "        fold_importance_df['importance'] = model.get_feature_importance()\n",
    "        fold_importance_df['fold'] = i+1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "          \n",
    "        del train_d, valid_d, model, fold_importance_df\n",
    "        gc.collect()\n",
    "        \n",
    "    print('====================')\n",
    "    print(f'CV AVG:\\n{loss} - {np.mean(valid_losses)}\\n{metric} - {np.mean(valid_metrics)}')\n",
    "    print(f'CV STD:\\n{loss} - {np.std(valid_losses)}\\n{metric} - {np.std(valid_metrics)}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return np.mean(valid_metrics)\n",
    "    else:\n",
    "        return oofs, models, feature_importance_df\n",
    "\n",
    "def cb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (16, 288), \n",
    "              'max_depth': (3, 16),\n",
    "              'subsample' : (0.1, 1),\n",
    "              'bagging_temperature' : (0, 100),\n",
    "              'colsample_bylevel': (0.001, 1),   \n",
    "              'min_data_in_leaf': (3, 700),\n",
    "              'l2_leaf_reg': (0.1, 300), \n",
    "              'random_strength': (0, 100)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=cb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_pred_clf(X_test, models, threshold):\n",
    "    y_test_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== CatBoost Predicting with {i+1}-th model ==========')\n",
    "        y_test = model.predict_proba(X_test)[:,1]\n",
    "        y_test_total += y_test\n",
    "    y_test_total /= len(models)\n",
    "    y_test = np.where(y_test_total > threshold, 1, 0)\n",
    "    gc.collect()\n",
    "    return y_test\n",
    "\n",
    "def lgb_pred_clf(X_test, models, threshold):\n",
    "    y_test_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== LightGBM Predicting with {i+1}-th model ==========')\n",
    "        y_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_total += y_test\n",
    "    y_test_total /= len(models)\n",
    "    y_test = np.where(y_test_total > threshold, 1, 0)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost (Regression: min RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_kfold_reg(X_train, y_train, category_cols, split, bayes_opt=True,\n",
    "                learning_rate=0.03, num_leaves=31, max_depth=6,\n",
    "                subsample=0.8, bagging_temperature=1.0, colsample_bylevel=1.0,\n",
    "                min_data_in_leaf=1, l2_leaf_reg=3.0, random_strength=1.0):\n",
    "    loss = 'RMSE'\n",
    "    metric = 'RMSE'\n",
    "    params = {'loss_function': loss,\n",
    "              'eval_metric': metric,\n",
    "              'boosting_type': 'Plain',\n",
    "              'random_seed': 8982,\n",
    "              'num_boost_round': 5000,\n",
    "              'early_stopping_rounds': 20,\n",
    "              'use_best_model': True,\n",
    "              # 'grow_policy': 'SymmetricTree','Depthwise','Lossguide',\n",
    "              'nan_mode': 'Max',\n",
    "              'od_type': 'Iter',\n",
    "              'verbose': 200,\n",
    "              \n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': subsample, #bf?\n",
    "              'bagging_temperature': bagging_temperature, #bf?\n",
    "              'colsample_bylevel': colsample_bylevel, #ff\n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'l2_leaf_reg': l2_leaf_reg,\n",
    "              'random_strength': random_strength}\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    models = []; valid_losses = []; valid_metrics = []\n",
    "    print(f'========== CatBoost Regressor training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        print(f'========== CatBoost Regressor training: {i+1}/{n_splits} ==========')\n",
    "        train_d = cb.Pool(data=X_train.loc[train_idx],\n",
    "                          label=y_train[train_idx],\n",
    "                          cat_features=category_cols)\n",
    "        valid_d = cb.Pool(data=X_train.loc[valid_idx],\n",
    "                          label=y_train[valid_idx],\n",
    "                          cat_features=category_cols)\n",
    "        \n",
    "        model = cb.CatBoostRegressor(**params)\n",
    "        model.fit(train_d, eval_set=valid_d)\n",
    "        \n",
    "        oofs[valid_idx] = model.predict(X_train.loc[valid_idx])\n",
    "        models.append(model)\n",
    "        # valid_losses.append(model.best_score_['validation'][f'{loss}'])\n",
    "        valid_metrics.append(model.best_score_['validation'][f'{metric}'])\n",
    "          \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = common_cols + category_cols\n",
    "        fold_importance_df['importance'] = model.get_feature_importance()\n",
    "        fold_importance_df['fold'] = i+1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "          \n",
    "        del train_d, valid_d, model, fold_importance_df\n",
    "        gc.collect()\n",
    "    \n",
    "    assert loss == metric\n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {np.mean(valid_metrics)}')\n",
    "    print(f'CV STD: {metric} - {np.std(valid_metrics)}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return -np.mean(valid_metrics)\n",
    "    else:\n",
    "        return oofs, models, feature_importance_df\n",
    "\n",
    "def cb_reg_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (16, 288), \n",
    "              'max_depth': (3, 16),\n",
    "              'subsample' : (0.1, 1),\n",
    "              'bagging_temperature' : (0, 100),\n",
    "              'colsample_bylevel': (0.001, 1),   \n",
    "              'min_data_in_leaf': (3, 700),\n",
    "              'l2_leaf_reg': (0.1, 300), \n",
    "              'random_strength': (0, 100)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=cb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', -optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = -optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_pred_reg(X_test, models):\n",
    "    y_test_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== CatBoost Predicting with {i+1}-th model ==========')\n",
    "        y_test = model.predict(X_test)\n",
    "        y_test_total += y_test\n",
    "    y_test_total /= len(models)\n",
    "    return y_test_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Binary Classification: max auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# encode categorical cols beforehand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_kfold_clf(X_train, y_train, split, bayes_opt=True,\n",
    "                  eta=0.3, gamma=0, max_depth=6, min_child_weight=1,\n",
    "                  subsample=0.7, colsample_bytree=1.0, colsample_bylevel=1.0,\n",
    "                  colsample_bynode=1.0, reg_lambda=1.0, reg_alpha=0.0):\n",
    "    metric = 'auc'\n",
    "    params = {'objective': 'binary:logistic',\n",
    "              'eval_metric': metric,\n",
    "              'booster': 'gbtree', # 'dart',\n",
    "              'seed': 8982,\n",
    "\n",
    "              'missing': np.nan,\n",
    "              # when dart: 'rate_drop': (0.0, 1.0),\n",
    "              # 'grow_policy': 'depthwise','lossguide',\n",
    "              # 'verbosity': 1, #0\n",
    "              # 'base_score': 0.5 <- initial leaf prediction\n",
    "              \n",
    "              'eta': eta,\n",
    "              'gamma': gamma, # pruning phase of split in XGBoost unique tree\n",
    "              'max_depth': int(max_depth),\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'subsample': subsample, # 0.5 - randomly sample half of the training data prior to growing trees\n",
    "              'colsample_bytree': colsample_bytree, # subsample ratio of columns when constructing each tree\n",
    "              'colsample_bylevel': colsample_bylevel, # subsample ratio of columns for each level\n",
    "              'colsample_bynode': colsample_bynode, # subsample ratio of columns for each node (split)\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'reg_alpha': reg_alpha}\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    \n",
    "    models = []; learning_curves = []; valid_losses = []; valid_metrics = []\n",
    "    print(f'========== XGBoost Classifier training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        print(f'========== XGBoost Classifier training: {i+1}/{n_splits} ==========')\n",
    "        train_d = xgb.DMatrix(data=X_train.loc[train_idx],\n",
    "                              label=y_train[train_idx],\n",
    "                              feature_names=common_cols+category_cols)\n",
    "                              #cat_features=category_cols)\n",
    "        valid_d = xgb.DMatrix(data=X_train.loc[valid_idx],\n",
    "                              label=y_train[valid_idx],\n",
    "                              feature_names=common_cols+category_cols)\n",
    "                              #cat_features=category_cols)\n",
    "        learning_curve = {}\n",
    "        model = xgb.train(params,\n",
    "                          train_d,\n",
    "                          evals=[(train_d, 'train'), (valid_d, 'valid')],\n",
    "                          num_boost_round=5000,\n",
    "                          early_stopping_rounds=20,\n",
    "                          verbose_eval=200,#False\n",
    "                          evals_result=learning_curve)\n",
    "        oofs[valid_idx] = model.predict(valid_d, ntree_limit=model.best_ntree_limit)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_metrics.append(model.best_score)\n",
    "          \n",
    "        del train_d, valid_d, model\n",
    "        gc.collect()\n",
    "        \n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {np.mean(valid_metrics)}')\n",
    "    print(f'CV STD: {metric} - {np.std(valid_metrics)}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return np.mean(valid_metrics)\n",
    "    else:\n",
    "        return oofs, models #, learning_curves\n",
    "\n",
    "def cb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'eta': (0.001, 0.3),\n",
    "              'gamma': (0, 10),\n",
    "              'max_depth': (3, 250),\n",
    "              'min_child_weight': (0, 100),\n",
    "              'subsample': (0.1, 1),\n",
    "              'colsample_bytree': (0.1, 1),\n",
    "              'colsample_bylevel': (0.1, 1),\n",
    "              'colsample_bynode': (0.1, 1),\n",
    "              'reg_lambda': (0, 300),\n",
    "              'reg_alpha': (0, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=cb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_pred_reg(X_test, models, threshold):\n",
    "    y_test_total = np.zeros(X_test.shape[0])\n",
    "    test_d = xgb.DMatrix(X_test)\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== XGBoost Predicting with {i+1}-th model ==========')\n",
    "        y_test = model.predict(test_d, ntree_limit=model.best_ntree_limit)\n",
    "        y_test_total += y_test\n",
    "    y_test_total /= len(models)\n",
    "    y_test = np.where(y_test_total > threshold, 1, 0)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Regression: max rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_kfold_reg(X_train, y_train, split, bayes_opt=True,\n",
    "                  eta=0.3, gamma=0, max_depth=6, min_child_weight=1,\n",
    "                  subsample=0.7, colsample_bytree=1.0, colsample_bylevel=1.0,\n",
    "                  colsample_bynode=1.0, reg_lambda=1.0, reg_alpha=0.0):\n",
    "    metric = 'rmse'\n",
    "    params = {'objective': 'reg:squarederror',\n",
    "              'eval_metric': metric,\n",
    "              'booster': 'gbtree', # 'dart',\n",
    "              'seed': 8982,\n",
    "\n",
    "              'missing': np.nan,\n",
    "              # when dart: 'rate_drop': (0.0, 1.0),\n",
    "              # 'grow_policy': 'depthwise','lossguide',\n",
    "              # 'verbosity': 1, #0\n",
    "              # 'base_score': 0.5 <- initial leaf prediction\n",
    "              \n",
    "              'eta': eta,\n",
    "              'gamma': gamma, # pruning phase of split in XGBoost unique tree\n",
    "              'max_depth': int(max_depth),\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'subsample': subsample, # 0.5 - randomly sample half of the training data prior to growing trees\n",
    "              'colsample_bytree': colsample_bytree, # subsample ratio of columns when constructing each tree\n",
    "              'colsample_bylevel': colsample_bylevel, # subsample ratio of columns for each level\n",
    "              'colsample_bynode': colsample_bynode, # subsample ratio of columns for each node (split)\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'reg_alpha': reg_alpha}\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    \n",
    "    models = []; learning_curves = []; valid_losses = []; valid_metrics = []\n",
    "    print(f'========== XGBoost Regressor training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        print(f'========== XGBoost Regressor training: {i+1}/{n_splits} ==========')\n",
    "        train_d = xgb.DMatrix(data=X_train.loc[train_idx],\n",
    "                              label=y_train[train_idx],\n",
    "                              feature_names=common_cols+category_cols)\n",
    "                              #cat_features=category_cols)\n",
    "        valid_d = xgb.DMatrix(data=X_train.loc[valid_idx],\n",
    "                              label=y_train[valid_idx],\n",
    "                              feature_names=common_cols+category_cols)\n",
    "                              #cat_features=category_cols)\n",
    "        learning_curve = {}\n",
    "        model = xgb.train(params,\n",
    "                          train_d,\n",
    "                          evals=[(train_d, 'train'), (valid_d, 'valid')],\n",
    "                          num_boost_round=5000,\n",
    "                          early_stopping_rounds=20,\n",
    "                          verbose_eval=200,#False\n",
    "                          evals_result=learning_curve)\n",
    "        oofs[valid_idx] = model.predict(valid_d, ntree_limit=model.best_ntree_limit)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_metrics.append(model.best_score)\n",
    "          \n",
    "        del train_d, valid_d, model\n",
    "        gc.collect()\n",
    "        \n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {np.mean(valid_metrics)}')\n",
    "    print(f'CV STD: {metric} - {np.std(valid_metrics)}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return -np.mean(valid_metrics)\n",
    "    else:\n",
    "        return oofs, models #, learning_curves\n",
    "\n",
    "def xgb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'eta': (0.001, 0.3),\n",
    "              'gamma': (0, 10),\n",
    "              'max_depth': (3, 250),\n",
    "              'min_child_weight': (0, 100),\n",
    "              'subsample': (0.1, 1),\n",
    "              'colsample_bytree': (0.1, 1),\n",
    "              'colsample_bylevel': (0.1, 1),\n",
    "              'colsample_bynode': (0.1, 1),\n",
    "              'reg_lambda': (0, 300),\n",
    "              'reg_alpha': (0, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=cb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', -optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = -optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_pred_reg(X_test, models):\n",
    "    y_test_total = np.zeros(X_test.shape[0])\n",
    "    test_d = xgb.DMatrix(X_test)\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== XGBoost Predicting with {i+1}-th model ==========')\n",
    "        y_test = model.predict(test_d, ntree_limit=model.best_ntree_limit)\n",
    "        y_test_total += y_test\n",
    "    y_test_total /= len(models)\n",
    "    return y_test_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neuralnet(\n",
    "    recipe,\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    lr=1e-3,\n",
    "    monitor='val_loss',\n",
    "    es_patience=-1,\n",
    "    restore_best_weights=True,\n",
    "    lr_scheduler='none',\n",
    "    lr_factor=0.1,\n",
    "    lr_patience=5,\n",
    "    seed=8982,\n",
    "    **_,\n",
    "):\n",
    "    tf.set_random_seed(seed)\n",
    "    model = keras.models.model_from_json(recipe)\n",
    "    \n",
    "    if loss == 'mse':\n",
    "        loss = keras.losses.mean_squared_error\n",
    "    elif loss == 'bce':\n",
    "        loss = keras.losses.binary_crossentropy\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(lr)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    callbacks = []\n",
    "    \n",
    "    if es_patience >= 0:\n",
    "        es = keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "                                           patience=es_patience,\n",
    "                                           restore_best_weights=restore_best_weights,\n",
    "                                           verbose=1)\n",
    "        callbacks.append(es)\n",
    "    \n",
    "    if lr_scheduler == 'none':\n",
    "        pass\n",
    "    elif lr_scheduler == 'reduce_on_plateau':\n",
    "        lr_sche = keras.callbacks.ReduceLROnPlateau(monitor=monitor,\n",
    "                                                    factor=lr_factor,\n",
    "                                                    patience=lr_patience,\n",
    "                                                    verbose=1)\n",
    "        callbacks.append(lr_sche)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return model, callbacks\n",
    "\n",
    "\n",
    "def train_neuralnet(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=None,\n",
    "):\n",
    "    model, callbacks = build_neuralnet(**params)\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              validation_data=validation_data,\n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=params['epochs'],\n",
    "              callbacks=callbacks)\n",
    "    return model\n",
    "\n",
    "def run_kfold_neuralnet(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    print(f\"k={n_splits} folds neuralnet running...\")\n",
    "    print(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        validation_data = [X_train.loc[val_idx, features], y_train[val_idx]]\n",
    "        model = train_neuralnet(params,\n",
    "                                X_train.loc[dev_idx, features],\n",
    "                                y_train[dev_idx],\n",
    "                                validation_data=validation_data)\n",
    "        \n",
    "        oof[val_idx] = model.predict(X_train.loc[val_idx, features].values)[:,0]\n",
    "        predictions += model.predict(X_test[features].values)[:,0] / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        print(msg)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    print(msg)\n",
    "\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat                0\n",
      "label              0\n",
      "cat_MEAN           0\n",
      "cat_SMTH_MEAN_1    0\n",
      "cat_SMTH_MEAN_5    0\n",
      "cat_MAX            0\n",
      "cat_MIN            0\n",
      "cat_RNG            0\n",
      "cat_STD            0\n",
      "cat_Q1             0\n",
      "cat_Q2             0\n",
      "cat_Q3             0\n",
      "cat_IQR            0\n",
      "dtype: int64\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_15 (Batc (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               3328      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "p_re_lu_10 (PReLU)           (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "p_re_lu_11 (PReLU)           (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 71,985\n",
      "Trainable params: 70,937\n",
      "Non-trainable params: 1,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "features_nn = list(set(x_tr.columns) - {'label'})\n",
    "x_tr['cat_STD'].fillna(0, inplace=True)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(len(features_nn,))),\n",
    "    keras.layers.BatchNormalization(),\n",
    "  \n",
    "    keras.layers.Dense(256),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "  \n",
    "    keras.layers.Dense(256),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "     keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat_SMTH_MEAN_5', 'cat_Q3', 'cat_MIN', 'cat_STD', 'cat_Q2', 'cat_RNG', 'cat_IQR', 'cat_MAX', 'cat_SMTH_MEAN_1', 'cat_MEAN', 'cat', 'cat_Q1']\n",
      "k=2 folds neuralnet running...\n",
      "train data/feature shape: (6, 12)\n",
      "Train on 2 samples, validate on 4 samples\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 553ms/sample - loss: 0.6931 - val_loss: 0.6915\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.6931 - val_loss: 0.6913\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 5ms/sample - loss: 0.6931 - val_loss: 0.6912\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.6931 - val_loss: 0.6910\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 4ms/sample - loss: 0.6931 - val_loss: 0.6909\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6907\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 12ms/sample - loss: 0.6931 - val_loss: 0.6906\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6905\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6904\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6902\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 11ms/sample - loss: 0.6931 - val_loss: 0.6899\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6897\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6896\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6894\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 7ms/sample - loss: 0.6931 - val_loss: 0.6892\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6890\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6888\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6886\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6883\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6881\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 15ms/sample - loss: 0.6931 - val_loss: 0.6879\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6878\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6876\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6875\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 10ms/sample - loss: 0.6931 - val_loss: 0.6874\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6873\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6871\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.6931 - val_loss: 0.6869\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 16ms/sample - loss: 0.6931 - val_loss: 0.6867\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 16ms/sample - loss: 0.6931 - val_loss: 0.6865\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6862\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6859\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6856\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6854\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6850\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6846\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6841\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6836\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 8ms/sample - loss: 0.6931 - val_loss: 0.6831\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6827\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6823\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 8ms/sample - loss: 0.6931 - val_loss: 0.6819\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6815\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6811\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6806\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6801\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 8ms/sample - loss: 0.6931 - val_loss: 0.6796\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6791\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 12ms/sample - loss: 0.6931 - val_loss: 0.6786\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 15ms/sample - loss: 0.6931 - val_loss: 0.6780\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6774\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6769\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6763\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 7ms/sample - loss: 0.6931 - val_loss: 0.6757\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6752\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 12ms/sample - loss: 0.6931 - val_loss: 0.6747\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6741\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 20ms/sample - loss: 0.6931 - val_loss: 0.6737\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.6931 - val_loss: 0.6732\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6728\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6723\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6719\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6713\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6708\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6703\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6699\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6694\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6689\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6684\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6679\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6674\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6669\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6664\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6659\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6654\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6650\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6646\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6642\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6638\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6633\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6629\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6625\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 17ms/sample - loss: 0.6931 - val_loss: 0.6621\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6617\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6612\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6608\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 4ms/sample - loss: 0.6931 - val_loss: 0.6604\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6600\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 17ms/sample - loss: 0.6931 - val_loss: 0.6596\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6592\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6588\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6584\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 6ms/sample - loss: 0.6931 - val_loss: 0.6580\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6576\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6571\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6566\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6562\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 6ms/sample - loss: 0.6931 - val_loss: 0.6558\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6554\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6550\n",
      "fold: 0 - auc_ome3: 0.62500\n",
      "Train on 4 samples, validate on 2 samples\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 293ms/sample - loss: 0.9460 - val_loss: 0.7050\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.4325 - val_loss: 0.7012\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5896 - val_loss: 0.6980\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.2837 - val_loss: 0.6957\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3961 - val_loss: 0.6944\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3579 - val_loss: 0.6936\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3193 - val_loss: 0.6933\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.4815 - val_loss: 0.6932\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.4868 - val_loss: 0.6932\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.4681 - val_loss: 0.6932\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 5ms/sample - loss: 0.2161 - val_loss: 0.6934\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3928 - val_loss: 0.6936\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 38ms/sample - loss: 0.3028 - val_loss: 0.6938\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3773 - val_loss: 0.6937\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3457 - val_loss: 0.6937\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3244 - val_loss: 0.6937\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 875us/sample - loss: 0.4321 - val_loss: 0.6937\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2776 - val_loss: 0.6936\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.4680 - val_loss: 0.6936\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2399 - val_loss: 0.6936\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.4140 - val_loss: 0.6935\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.4374 - val_loss: 0.6934\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2938 - val_loss: 0.6934\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.4666 - val_loss: 0.6933\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3460 - val_loss: 0.6933\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 875us/sample - loss: 0.2559 - val_loss: 0.6932\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2701 - val_loss: 0.6932\n",
      "Epoch 28/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "4/4 [==============================] - 0s 45ms/sample - loss: 0.3670 - val_loss: 0.6932\n",
      "Epoch 00028: early stopping\n",
      "fold: 1 - auc_ome3: 0.50000\n",
      "CV score - auc_ome3: 0.66667\n"
     ]
    }
   ],
   "source": [
    "SEED = 8982\n",
    "\n",
    "params = {\n",
    "    'recipe': model.to_json(),\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 1e-3,\n",
    "    'loss': 'bce',\n",
    "    'monitor': 'val_loss',\n",
    "    'es_patience': 20,\n",
    "    'restore_best_weights': True,\n",
    "    'lr_scheduler': 'reduce_on_plateau',\n",
    "    'lr_factor': 0.1,\n",
    "    'lr_patience': 5,\n",
    "    'seed': SEED,\n",
    " \n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "print(features_nn)\n",
    "X_train_nn = x_tr[features_nn]\n",
    "y_train_nn = x_tr['label'].values\n",
    "X_test_nn = x_te[features_nn]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "metrics = {'auc_ome3': roc_auc_score}\n",
    "\n",
    "seed_list = SEED + np.arange(3)\n",
    "oof_nn, predictions_nn = run_kfold_neuralnet(params,\n",
    "                                               X_train_nn,\n",
    "                                               y_train_nn,\n",
    "                                               X_test_nn,\n",
    "                                             build_cv_spliter(X_train_nn, y_train_nn, n_splits=2),\n",
    "                                               features_nn,\n",
    "                                               metrics)\n",
    "                                               #seed_list=seed_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
