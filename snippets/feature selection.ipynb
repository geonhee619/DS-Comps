{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(\n",
    "    feature_importance_df,\n",
    "    feature_name='feature',\n",
    "    importance_name=['split', 'gain'],\n",
    "    top_k=50,\n",
    "    fig_width=16,\n",
    "    fig_height=8,\n",
    "    fontsize=14,\n",
    "):\n",
    "    if isinstance(importance_name, str):\n",
    "        importance_name = [importance_name]\n",
    "    \n",
    "    num_importance = len(importance_name)\n",
    "    plt.figure(figsize=(fig_width, fig_height*num_importance))\n",
    "    gs = gridspec.GridSpec(1, num_importance)\n",
    "    \n",
    "    def _fetch_best_features(df, fimp='gain'):\n",
    "        cols = (df[[feature_name, fimp]]\n",
    "                .groupby(feature_name)\n",
    "                .mean()\n",
    "                .sort_values(by=fimp, ascending=False)\n",
    "                .index\n",
    "                .values[:top_k])\n",
    "        return cols, df.loc[df[feature_name].isin(cols)]\n",
    "    \n",
    "    for i, fimp in enumerate(importance_name):\n",
    "        cols, best_features = _fetch_best_features(feature_importance_df, fimp)\n",
    "        ax = plt.subplot(gs[0, i])\n",
    "        sns.barplot(x=fimp, y=feature_name, data=best_features, order=cols, ax=ax)\n",
    "        title = f'Features {fimp} importance (averaged/folds)'\n",
    "        plt.title(title, fontweight='bold', fontsize=fontsize)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# or this\n",
    "# fold_importance_df.plot.barh(x='feature', y='gain', figsize=(13,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Feature Elimination by LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iterative_CV:\n",
    "    def __init__(self, X_train_full, y_train, eval_cols, metric):\n",
    "        self.X_train_full = X_train_full\n",
    "        self.y_train = y_train\n",
    "        self.eval_cols = eval_cols\n",
    "        self.metric = metric\n",
    "    \n",
    "    # =====\n",
    "    # eliminates/imputes one feature at a time,\n",
    "    # returns list with options and discards\n",
    "    # ====\n",
    "    \n",
    "    def iter_cv_elim():\n",
    "        excl_improve = []; excl_worse = []\n",
    "        if self.metric == 'rmse':\n",
    "            init_valid_avg_score = -1 * lgb_kfold(self.X_train_full, self.y_train, bayes_opt=True)\n",
    "            print(f'[Iter_Feature_Elim] Current best score is {init_valid_avg_score}')\n",
    "            for cols in tqdm(self.eval_cols):\n",
    "                temp_cols = list(set(self.X_train_full.columns) - {col})\n",
    "                X_train = self.X_train_full[temp_cols]\n",
    "                new_valid_avg_score = -1 * lgb_kfold(X_train, self.y_train, bayes_opt=True)\n",
    "                degree = new_valid_avg_score - init_valid_avg_score\n",
    "                if degree < 0:\n",
    "                    pct = 100 * (-1 * degree / init_valid_avg_score)\n",
    "                    excl_improve.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion improved (lowered) avg CV by {pct}pct.\")\n",
    "                else:\n",
    "                    pct = 100 * (degree / init_valid_avg_score)\n",
    "                    excl_worse.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion worsened (raised) avg CV by {pct}pct.\")\n",
    "        elif self.metric == 'auc':\n",
    "            init_valid_avg_score = lgb_skfold(self.X_train_full, self.y_train, bayes_opt=True)\n",
    "            print(f'[Iter_Feature_Elim] Current best score is {init_valid_avg_score}')\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                temp_cols = list(set(self.X_train_full.columns) - {col})\n",
    "                X_train = self.X_train_full[temp_cols]\n",
    "                new_valid_avg_score = lgb_skfold(X_train, self.y_train, bayes_opt=True)\n",
    "                degree = new_valid_avg_score - init_valid_avg_score\n",
    "                if degree > 0:\n",
    "                    pct = 100 * (degree / init_valid_avg_score)\n",
    "                    excl_improve.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion improved (raised) avg CV by {pct}pct.\")\n",
    "                else:\n",
    "                    pct = 100 * (-1 * degree / init_valid_avg_score)\n",
    "                    excl_worse.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion worsened (lowered) avg CV by {pct}pct.\")\n",
    "\n",
    "        excl_improve.sort(key=lambda lst: lst[1])\n",
    "        excl_worse.sort(key=lambda lst: lst[1])\n",
    "        del init_valid_avg_score, cols, temp_cols, X_train, new_valid_avg_score, degree, pct\n",
    "        gc.collect()\n",
    "        return excl_improve, excl_worse\n",
    "    \n",
    "    def iter_cv_rank():\n",
    "        impt = []\n",
    "        if self.metric == 'rmse':\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                X_train = self.X_train_full[col]\n",
    "                assert X_train.shape[1] == 1\n",
    "                print(f\"[Iter_Feature_Rank] '{col}', evaluation ongoing.\")\n",
    "                valid_avg_score = -1 * lgb_kfold(X_train, self.y_train, bayes_opt=True)\n",
    "                impt.append([col, valid_avg_score])\n",
    "        elif self.metric == 'auc':\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                X_train = self.X_train_full[col]\n",
    "                assert X_train.shape[1] == 1\n",
    "                print(f\"[Iter_Feature_Rank] '{col}', evaluation ongoing.\")\n",
    "                valid_avg_score = lgb_skfold(X_train, self.y_train, bayes_opt=True)\n",
    "                impt.append([col, valid_avg_score])\n",
    "        impt.sort(key=lambda lst: lst[1])\n",
    "        del col, X_train, valid_avg_score\n",
    "        gc.collect()\n",
    "        return impt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Importance Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lgb_fimp(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    shuffle,\n",
    "    seed=42,\n",
    "    categorical=[]\n",
    "):\n",
    "    # Shuffle target if required\n",
    "    y = y_train.copy()\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        y = y_train.copy().sample(frac=1.0)\n",
    "    \n",
    "    arg_categorical = categorical if len(categorical) > 0 else 'auto'\n",
    "    dtrain = lgb.Dataset(X_train[features],\n",
    "                         label=y.values,\n",
    "                         categorical_feature=arg_categorical)\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params, dtrain)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = features\n",
    "    imp_df['split'] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['gain'] = clf.feature_importance(importance_type='gain')\n",
    "    \n",
    "    return imp_df\n",
    "\n",
    "\n",
    "def null_importance_selection(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    seed=42,\n",
    "    categorical=[],\n",
    "    num_actual_run=1,\n",
    "    num_null_run=40,\n",
    "    eps=1e-10,\n",
    "    valid_percentile=75,\n",
    "):\n",
    "    actual_imp_df = pd.DataFrame()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for i in tqdm(range(num_actual_run)):\n",
    "        seed = np.random.randint(1000)\n",
    "        imp_df = _get_lgb_fimp(params,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               features,\n",
    "                               shuffle=False,\n",
    "                               seed=seed,\n",
    "                               categorical=categorical)\n",
    "        imp_df['run'] = i\n",
    "        actual_imp_df = pd.concat([actual_imp_df, imp_df], axis=0)\n",
    "    \n",
    "    null_imp_df = pd.DataFrame()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for i in tqdm(range(num_null_run)):\n",
    "        seed = np.random.randint(1000)\n",
    "        imp_df = _get_lgb_fimp(params,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               features,\n",
    "                               shuffle=True,\n",
    "                               seed=seed,\n",
    "                               categorical=categorical)\n",
    "        imp_df['run'] = i\n",
    "        null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    \n",
    "    feature_scores = []\n",
    "    \n",
    "    for _f in actual_imp_df['feature'].unique():\n",
    "        # importance gain of gain\n",
    "        act_fimp_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'split'].mean()\n",
    "        null_fimp_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'split'].values\n",
    "        split_score = np.log(eps + act_fimp_split / (1 + np.percentile(null_fimp_split, valid_percentile)))\n",
    "        \n",
    "        # importance gain of gain\n",
    "        act_fimp_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'gain'].mean()\n",
    "        null_fimp_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'gain'].values\n",
    "        gain_score = np.log(eps + act_fimp_gain / (1 + np.percentile(null_fimp_gain, valid_percentile)))\n",
    "\n",
    "        feature_scores.append((_f, split_score, gain_score))\n",
    "    \n",
    "    scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_high_corr_columns(df, threshold=0.99, verbose=True):\n",
    "    df_corr = abs(df.corr())\n",
    "    delete_columns = []\n",
    "    \n",
    "    # diagonal values filled by zero\n",
    "    for i in range(0, len(df_corr.columns)):\n",
    "        df_corr.iloc[i, i] = 0\n",
    "    \n",
    "    # loop as removing high-correlated columns in df_corr\n",
    "    while True:\n",
    "        df_max_column_value = df_corr.max()\n",
    "        max_corr = df_max_column_value.max()\n",
    "        query_column = df_max_column_value.idxmax()\n",
    "        target_column = df_corr[query_column].idxmax()\n",
    "        \n",
    "        if max_corr < threshold:\n",
    "            break\n",
    "        else:\n",
    "            # drop feature which is highly correlated with others \n",
    "            if sum(df_corr[query_column]) <= sum(df_corr[target_column]):\n",
    "                delete_column = target_column\n",
    "                saved_column = query_column\n",
    "            else:\n",
    "                delete_column = query_column\n",
    "                saved_column = target_column\n",
    "            \n",
    "            df_corr.drop([delete_column], axis=0, inplace=True)\n",
    "            df_corr.drop([delete_column], axis=1, inplace=True)\n",
    "            delete_columns.append(delete_column)\n",
    "            \n",
    "            if verbose:\n",
    "                printl('{}: Drop: {} <- Query: {}, Corr: {:.5f}'.format(\n",
    "                    len(delete_columns), delete_column, saved_column, max_corr\n",
    "                ))\n",
    "\n",
    "    return delete_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
